# 20240608
## AI 드론봇 - 6

#### 합성곱 신경망 II

##### 이미지 분류를 위한 신경망
입력 데이터로 이미지를 사용한 분류는 특정 대상이 영상 내에 존재하는지 여부를 판단하는 것  

###### LeNet-5
합성곱 신경망이라는 개념을 최초로 얀 르쿤이 개발한 구조  

합성곱과 다운 샘플링(풀링)을 반복적으로 거치면서 마지막 완전연결층에서 분류를 수행함  

``` python
# pip install --user tqdm

# 필요한 라이브러리 호출
import torch
import torchvision
from torch.utils.data import DataLoader, Dataset
# 이미지 변환 기능을 제공하는 라이브러리
from torchvision import transforms
from torch.autograd import Variable
# 경사 하강법을 이용하여 가중치를 구하기 위한 옵티마이저 라이브러리
from torch import optim
import torch.nn as nn
import torch.nn.functional as F
import os
import cv2
# 진행 상황을 가시적으로 표현해주는 특히 모델의 학습 경과를 확인하고 싶을 때 사용하는 라이브러리
from tqdm import tqdm
import random
from matplotlib import pyplot as plt
# PIL는 이미지 처리와 조작을 위한 강력한 라이브러리
from PIL import Image

# 텐서플로는 gpu를 자동으로 할당함
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# 이미지 데이터셋 전처리

class ImageTransform():
    
    def __init__(self, resize, mean, std):
        self.data_transform = {
            'train': transforms.Compose([
                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(mean, std)
            ]),
            'val': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(resize),
                transforms.ToTensor(),
                transforms.Normalize(mean, std),
            ])
        }
    
    def __call__(self, img, phase):
        return self.data_transform[phase](img)

# 이미지 데이터셋을 부러온 후 훈련 검증, 테스트로 분리

cat_directory = r'./data/dogs-vs-cats/Cat/'
dog_directory = r'./data/dogs-vs-cats/Dog/'

# os.listdir: 지정한 디렉터리 내 모든 파일의 리스트 반환
cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])
dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])

# 개와 고양이 이미지를 합쳐서 image_filepaths에 저장
images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]
correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]

# 특정 seed 값을 부여하면 상태가 저장되기에 동일한 난수를 생성함
random.seed(42)
random.shuffle(correct_images_filepaths)
#train_images_filepaths = correct_images_filepaths[:20000] #성능을 향상시키고 싶다면 훈련 데이터셋을 늘려서 테스트해보세요   
#val_images_filepaths = correct_images_filepaths[20000:-10] #훈련과 함께 검증도 늘려줘야 합니다
# 훈련용 400개 이미지
train_images_filepaths = correct_images_filepaths[:400]
# 검증용 92개의 이미지
val_images_filepaths = correct_images_filepaths[400:-10]
# 테스트용 10개의 이미지
test_images_filepaths = correct_images_filepaths[-10:]
print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))
```

`torchvision` 라이브러리를 통해 이미지 전처리를 손쉽게 할 수 있다.  

['torchvision.transforms'에서 사용하는 파라미터]  
- transforms.Compose: 이미지를 변형할 수 있는 방식의 묶음
- transfroms.RandomResizeCrop: 이미지를 주어진 크기로 조정함, resize는 224 x 224를 나타냄, scale은 원래 이미지를 임의의 크기만큼 면적을 무작위로 자르겠다는 의미
- transforms.RandomHorizontalFlip: 주어진 확률로 이미지를 수평 반전시킴, 파라미터 값을 넣으면 해당 확률만큼 뒤집기를 수행함  
- transforms.ToTensor: ImageFolder 메서드를 비롯해 torchvision 메서드는 이미지를 읽을 때 파이썬 이미지 라이브러리인 PIL를 사용함, 효율적인 연산을 위해서 `torch.FloatTensor`  배열로 바꾸어야 하는데 이 때 픽셀 값의 범위를 [0.0, 1.0]으로 바꾸고 차원의 순서도 C x H x W로 바꿈
- transforms.Normalize: 전이 학습에 사용하는 사전 훈련된 모델은 대개 ImageNet 데이터셋에서 훈련, RGB 채널마다 평균과 표준편차에 따라서 정규화를 해주어야 함, 참고로 읽어오는 방법에 따라서 채널 순서에 주의해야 함
- `__call__`는 인스턴스가 호출되었을 때, 발동

`plt.subplot`: 각각의 서브 플롯을 설정할 때마다 호출해야 함  
`plt.subplots`: 한 번에 여러 서브 플롯을 설정할 수 있음  
`fig`: 전체 subplot을 가리킴  
`ax`: 낱개 각각을 가리킴(a1, a2 등으로 가리킴)


``` python
# 난수 생성을 위한 seed 조정

import numpy as np

np.random.seed(101)
np.random.randint(low=1, high=10, size=10)
# array([2, 7, 8, 9, 5, 9, 6, 1, 6, 9])

np.random.seed(100)
np.random.randint(low=1, high=10, size=10)
# array([9, 9, 4, 8, 8, 1, 5, 3, 6, 3])

np.random.seed(101)
np.random.randint(low=1, high=10, size=10)
# array([2, 7, 8, 9, 5, 9, 6, 1, 6, 9])
```

``` python
# 테스트 데이터셋 이미지 확인 함수

def display_image_grid(images_filepaths, predicted_labels=(), cols=5):
    rows = len(images_filepaths) // 5
    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))
    for i, image_filepath in enumerate(images_filepaths):
        image = cv2.imread(image_filepath)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # 이미지의 전체 경로를 정규화하고 분할을 위한 코드
        true_label = os.path.normpath(image_filepath).split(os.sep)[-2]
        # predicted_label에 대한 값을 정의하기 위한 코드
        predicted_label = predicted_labels[i] if predicted_labels else true_label
        # 예측과 정답(레이블)이 동일하면 초록색으로 표시하고, 그렇지 않다면 빨간색으로 표시
        color = 'green' if true_label == predicted_label else 'red'
        # 개별 이미지 출력
        ax.ravel()[i].imshow(image)
        # predicted_label을 타이틀로 사용
        ax.ravel()[i].set_title(predicted_label, color=color)
        # 이미지의 축 제거
        ax.ravel()[i].set_axis_off()
    # 이미지의 여백을 제거
    plt.tight_layout()
    plt.show()
```

`cv2.cvtColor`는 이미지의 색상을 변경하기 위해 사용  
[파라미터]  
- 첫 번째 파라미터: 입력 이미지
- 두 번째 파라미터: 변환할 이미지의 색상을 지정하는 것으로 BRG(Blue, Green, Red)채널 이미지를 RGB(컬러)로 변환하겠다는 의미

`os.path.normpath`: 경로명을 정규화, A//B, A/B/, A/./B 등등 모두 A/B로 경로를 통일  
`split(os.sep)`: 경로를 / 혹은 \을 기준으로 분할할 때 사용  
ex. c:/temp/user/a.jpg라는 경로를 split(os.sep)을 하면 ['c:', 'temp', 'user', 'a.jpg']처럼 분할됨  

`ax.ravel()`는 다차원 배열을 1차원으로 평탄화하는 메서드

``` python
# 테스트 데이터셋 이미지를 출력
display_image_grid(test_images_filepaths)
```

모델 학습을 위한 구체적인 단계  
1. 데이터셋에는 학습할 데이터의 경로를 정의하고, 그 경로에서 데이터를 읽어 옴
2. 데이터로그에서 데어테셋의 데이터를 메모리로 불러옴(배치 크기만큼 분할해서 가져 옴)

다수와 개의 고양이 이미지가 포함된 데이터에서 이들을 예측하는 것이 이번 목표  

``` python
# 이미지 데이터셋 클래스 정의

class DogvsCatDataset(Dataset):

    # 데어터셋의 전처리
    def __init__(self, file_list, transform=None, phase='train'):
        self.file_list = file_list
        # DogvsCatDataset 클래스를 호출할 때 transform에 대한 매개 변수를 받아 옴
        self.transform = transform
        # train 적용
        self.phase = phase

    # images_filepaths 데이터셋의 전체 길이를 반환
    def __len__(self):
        return len(self.file_list)

    # 데이터셋에서 데이터를 가져오는 부분으로 결과는 텐서 형태
    def __getitem__(self, idx):
        img_path = self.file_list[idx]
        img = Image.open(img_path)
        # 이미지에 'train' 전처리를 적용
        img_transformed = self.transform(img, self.phase)
        label = img_path.split('/')[-1].split('.')[0]
        if label == 'dog':
            label =  1
        elif label == 'cat':
            label = 0
        return img_transformed, label

# 변수 값 정의

size = 224
mean = (0.485, 0.456, 0.406)
std = (0.229, 0.224, 0.225)
batch_size = 32

# 이미지 데이터셋 정의

# 훈련 이미지에 train_transforms를 적용
train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')
val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')

index = 0
# 훈련 데이터(train_dataset.__getitem__[0][0])의 크기를 출력
print(train_dataset.__getitem__(index)[0].size())
# 훈련 데이터의 레이블 출력
print(train_dataset.__getitem__(index)[1])

# 데이터로더 정의

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
# 훈련 데이터셋과 검증 데이터셋을 합쳐서 표현
dataloader_dict = {'train': train_dataloader, 'val': val_dataloader}

batch_iterator = iter(train_dataloader)
inputs, label = next(batch_iterator)
print(inputs.size())
print(label)

# 모델의 네트워크 클래스

class LeNet(nn.Module):

    def __init__(self):
        super(LeNet, self).__init__()
        # 2D 합성곱층이 적용, 입력 형태는 (3, 224, 224), 출력 형태는 (weight-kernel_size+1) / stride에 따라 (16, 220, 220)
        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0)
        # ReLU는 활성화 함수, 새로운 텐서를 만들겠다.
        self.relu1 = nn.ReLU()
        # 최대 풀링이 적용됨, 적용 이후 출력 형태는 220/2가 되어 (16, 110, 110)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        # 2D 합성곱층이 적용, 출력 형태는 (32, 106, 106)
        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)
        self.relu2 = nn.ReLU()
        # 최대 풀링이 적용, 출력 형태는 (32, ,53, 53)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        self.fc1 = nn.Linear(32*53*53, 512)
        self.relu5 = nn.ReLU()
        self.fc2 = nn.Linear(512, 2)
        self.output = nn.Softmax(dim=1)

    def forward(self, x):
        out = self.cnn1(x)
        out = self.relu1(out)
        out = self.maxpool1(out)
        out = self.cnn2(out)
        out = self.relu2(out)
        out = self.maxpool2(out)
        # 완전 연결층에 데이터를 전달하기 위해 데이터 형태를 1차원으로 바꿈, self가 아닌 out에서 view를 호출
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        out = self.fc2(out)
        # 마지막 레이어에는 확률 계산에 부정적인 영향을 끼치는 것을 방지하기 위해 ReLU 대신 Softmax를 사용하는 것이 일반적
        out = self.output(out)
        return out
```

파이토치의 데이터로더는 배치 관리를 담당, 조금씩 나눠서 메모리에 데이터를 불러옴  
`train_dataloader = DataLoader(trian_dataset, batch_size=batch_size, shuffle=True)`  
- 첫 번째 파라미터: 데이터를 불러오기 위한 데이터셋  
- batch_size: 한 번에 메모리로 불러올 데이터 크기
- shuffle: 메모리를 데이터로 가져올 때 임의로 섞어서 가져오는지 여부

Conv2d 계층에서 출력 크기를 구하는 공식: (입력 데이터의 크기 - 커널 크기 + 2 * 패딩 크기) / 스트라이드 + 1  
MaxPool2d 계층에서 출력 크기를 구하는 공식: 입력 필터의 크기 / 커널 크기

``` python
# 모델 객체 생성
model = LeNet().to(device)
print(model)

# torchsummary 라이브러리를 이용한 모델의 네트워크 구조 확인
# 케라스와 같은 형태로 모델을 출력할 수 있는 라이브러리

# >pip install torchsummary
from torchsummary import summary

summary(model, input_size=(3, 224, 224))
```

`summary`: 모델의 네트워크 관련 정보 확인  
[파라미터]  
- 첫 번째 파라미터: 모델의 네트워크
- 두 번째 파라미터: (채널, 너비, 높이)

``` python
# 학습 가능한 파라미터 수 확인

def count_parameters(model):
    # p: 파라미터를 나타내는 변수
    # p.numel(): 파라미터의 요소 수를 반환하는 메서드, 3x3 크기의 경우 9개의 요소가 된다.
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')

# 옵티마이저와 손실 함수 정의
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
critertion = nn.CrossEntropyLoss()
```
모멘텀 SGD: SGD에 관성이 추가된 것, 매번 기울기를 구하지만, 가중치를 수정하기 전에 이전 수정 방향을 참고하여 같은 방향으로 일정한 비율만 수정하는 방법  

[파라미터]  
- 첫 번째 파라미터: 경사 하강법을 통해 궁극적으로 업데이터하고자 하는 파라미터
- lr: 가중치를 변경할 때마다 얼마나 크게 변경할 지
- momentum: SGD를 적절한 방향으로 가속하며 흔들림을 줄여주는 매개 변수

``` python
# 모델 학습 함수 정의

def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):
    since = time.time()
    best_acc = 0.0

    for epoch in range(num_epoch):
        print(f'Epoch {epoch+1}/{num_epoch}')
        print('-' * 20)

        for phase in ['train', 'val']:
            if phase == 'train':
                # 모델을 학습
                model.train()
            else:
                model.eval()
            
            epoch_loss = 0.0
            epoch_corrects = 0

            # dataloader_dict는 훈련 데이터셋을 의미
            for inputs, labels in tqdm(dataloader_dict[phase]):
                # inputs = inputs.to(device)
                # labels = labels.to(device)
                # 역전파 단계를 실행하기 전에 기울기를 0으로 초기화
                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    # 손실 함수를 이용한 오차 계산
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        # 모델의 학습 가능한 모든 파라미터에 대해 기울기를 계산
                        loss.backward()
                        # optimizer의 step 함수를 호출하면 파라미터를 갱신
                        optimizer.step()
                    
                    print(loss.item())
                    # 오차와 입력을 곱함
                    epoch_loss += loss.item() * inputs.size(0)
                    # 정답과 예측이 일치하면 그것의 합계를 epoch_corrects에 저장
                    epoch_corrects += torch.sum(preds == labels.data)
            
            # 최종 오차 계산(오차를 데이터셋의 길이(개수)로 나누어서 계산)
            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)
            # 최종 정확도(epoch_corrects를 데이터셋의 길이(개수)로 나누어서 계산), 정확도는 소수점까지 표현
            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # 검증 데이터셋에 대한 가장 최적의 정확도를 저장
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = model.state_dict()

    time_elapsed = time.time() - since
    print(f'Training complete in {time:.0f}m {time:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')
    return model
```
`criterion = nn.CrossEntropyLoss(reduction='mean')`: reduction 파라미터의 기본 값은 mean, 정답과 예측 값의 오차를 구한 후 그 값의 평균을 반환함  
`mean`은 정답과 예측 값의 오차를 구한 후, 값의 평균을 반환함  


``` python
# 모델 테스트를 위한 함수 정의

import pandas as pd

id_list, pred_list = [], []
_id = 0

# 역전파 중 텐서에 대한 변화도를 계산할 필요가 없음을 나타내는 것으로 훈련 데이터셋 모델 학습과 가장 큰 차이
# with: 컨텍스트 관리자, 파일, 네트워크 연결, 데이터베이스 연결 등 리소스를 열고 닫는 작업에서 사용
with torch.no_grad():
    # 테스트 데이터셋 이용
    for test_path in tqdm(test_images_filepaths):
        img = Image.open(test_path)
        # 변수 이름 앞의 밑줄은 관례적으로 해당 변수를 내부적으로 활용하거나 특별한 의미를 가지지 않음을 나타냄
        # 이진 분류 문제에서 클래스 1의 확률을 얻기 위해 두 번째 차원을 가준으로 softmax를 적용함
        _id = test_path.split('/')[-1].split('.')[1]
        transform = ImageTransform(size, mean, std)
        # 테스트 데이터셋 전처리 적용
        img = transform(img, phase='val')
        img = img.unsqueeze(0)
        img = img.to(device)

        model.eval()
        outputs = model(img)
        preds = F.softmax(outputs, dim=1)[:, 1].tolist()
        id_list.append(_id)
        pred_list.append(preds[0])

# 테스트 데이터셋의 예측 결과인 id와 레이블을 데이터 프레임에 저장
res = pd.DataFrame({
    'id': id_list,
    'label': pred_list,
})

res.sort_values(by='id', inplace=True)
# 기존의 인덱스를 삭제하고, 새로운 순차적인 인덱스를 생성하도록 함
res.reset_index(drop=True, inplace=True)

# 데이터 프레임을 CSV 파일로 저장
res.to_csv('./data/LeNet', index=False)
```

`torch.unsqueeze`는 텐서에 차원을 추가할 때 사용, 매개변수는 차원이 추가될 인덱스를 의미함  
`softmax`는 지정된 차원을 따라 텐서의 요소가 (0, 1) 범위에 있고 합계가 1이 되도록 크기를 다시 조정함  

`F.softmax(outputs, dim=1)`을 통해 각 행의 합이 1이 되도록 함 `[:, 1]`을 통해 모든 행에서 첫 번째 인덱스를 가져옴, `tolist()`를 통해 리스트 형태로 변환함  

``` python
# 테스트 데이셋의 예측 결과 호출

# 예측 결과 레이블이 0.5보다 크면 개, 0.5보다 작으면 나비
# model 함수에서 out = self.output(out)에서 out은 클래스 별 확률을 나타내는 텐서이고, 개의 확률을 preds에 저장하므로 이를 통해 0.5를 기준으로 식별 가능함
res.head(10)
```

###### AlexNet
CNN은 3차원 구조를 갖는다.  
색상이 많은 이미지는 R / G / B 성분 3개를 갖기 때문에 채널이 3개.  
하지만, 합성곱을 거치면서 특성 맵이 만들어지고, 중간 영상의 깊이가 달라짐  

AlexNet는 합성곱 층 5개와 완전연결층 3개로 이루어져 있으며, 맨 마지막 완전연결층은 카테고리 1000개를 분류하기 위해 소프트맥스 활성화 함수를 사용하고 있음 + GPU를 두 개 사용하는 것 외에는 LeNet과 차이가 있지는 않음  

성능이 좋은 결과를 얻기 위해서는 충분한 데이터셋을 확보하는 것뿐만 아니라 전처리 부분에서 데이터를 많이 확장(RandomRotation, RandomHoriziontalFlip 등을 이용하여 예제를 진행해야 함)

`AvgPool2d`: 풀링에 대한 커널 및 스트라이드 크기를 정의해야 동작함  
`AdaptiveAvgPool2d`: 풀링작업이 끝날 때 필요한 출력 크기만 정의함, 입력 크기에 변동이 있고 CNN 위쪽에 완전 연결층을 사용하는 경우 유용함  

``` python
# 필요한 라이브러리 호출

import torch
import torchvision
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torch.autograd import Variable
from torch import optim
import torch.nn as nn
import torch.nn.functional as F
import os
import cv2
import random
from PIL import Image
from tqdm import tqdm
import time
import matplotlib.pyplot as plt

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# 데이터 전처리

from typing import Any


class ImageTransform():
    
    def __init__(self, resize, mean, std):
        self.data_transform = {
            'train': transforms.Compose([
                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(mean, std)
            ]),
            'val': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(resize),
                transforms.ToTensor(),
                transforms.Normalize(mean, std)
            ])
        }
    
    def __call__(self, img, phase):
        return self.data_transform[phase](img)

# 데이터를 가져와서 훈련, 검증, 테스트 용도로 분리

cat_directory = './data/dogs-vs-cats/Cat/'
dog_directory = './data/dogs-vs-cats/Dog/'
human_directory = './data/dogs-vs-cats/Test/'

cat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])
dog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])
human_images_filepaths = sorted([os.path.join(human_directory, f) for f in os.listdir(human_directory)])
images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]

correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]

random.seed(42)
random.shuffle(correct_images_filepaths)
train_images_filepaths = correct_images_filepaths[:4000]
val_images_filepaths = correct_images_filepaths[4000: 4920]
test_images_filepaths = [*correct_images_filepaths[-10:], *human_images_filepaths]
print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))

# torch.utils.data.Dataset을 상속받아 커스텀 데이터셋을 정의

class DogvsCatDataset(Dataset):

    def __init__(self, file_list, transform=None, phase='train'):
        # 이미지 데이터가 위치한 파일 경로, 채널이 3개인 이미지를 필터링
        self.file_list = file_list
        self.file_list = [f for f in self.file_list if Image.open(f).mode == 'RGB']
        # 이미지 데이터 전처리
        self.transform = transform
        # self.phase는 ImageTransform()에서 정의한 'train'과 'val'을 의미
        self.phase = phase
    
    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        # 이미지 데이터의 인덱스를 가져오기
        img_path = self.file_list[idx]
        img = Image.open(img_path)
        img_transformed = self.transform(img, self.phase)

        # 레이블 값을 가져오기
        label = img_path.split('/')[-1].split('.')[0]
        if label == 'dog':
            label = 1
        elif label == 'cat':
            label = 0

        # 전처리가 적용된 이미지와 레이블 반환
        return img_transformed, label

# 변수에 대한 값 정의

# AlexNet은 깊이가 깊은 네트워크를 사용하므로 이미지 크기가 256이 아니면 풀링충 때문에 크기가 계속 줄어들어 오류가 발생할 수 있음

size = 256
# 파이토치의 사전 학습된 모델을 사용할 때, 모델이 학습된 데이터셋의 통계를 기반으로 함
mean = (0.485, 0.456, 0.406)
std = (0.229, 0.224, 0.225)
# 배치 사이즈는 gpu의 메모리 용량에 따라서 달리하는 것이 좋다.
batch_size = 64

# 훈련, 검증, 테스트 데이터셋 정의

train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')
val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')
test_dataset = DogvsCatDataset(test_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')

index = 0
print(train_dataset.__getitem__(index)[0].size())
print(train_dataset.__getitem__(index)[1])

# 데이터셋을 메모리로 불러옴

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
dataloader_dict = {
    'train': train_dataloader,
    'val': val_dataloader,
}

batch_iterator = iter(train_dataloader)
inputs, label = next(batch_iterator)
print(inputs.size())
# 배치 사이즈가 커지니까 label의 크기도 커졌다.
print(label)

# AlexNet 모델 네트워크 정의

class AlexNet(nn.Module):

    def __init__(self) -> None:
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),
        )

        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256*6*6, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 2),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

# model 객체 생성

model = AlexNet()
model.to(device)
# 옵티마이저 및 손실 함수 정의

optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
criterion = nn.CrossEntropyLoss()

# 모델 네트워크 구조 확인
from torchsummary import summary

summary(model, input_size=(3, 256, 256))

# 모델 학습 함수 정의

def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):
    since = time.time()
    best_acc = 0.0

    for epoch in range(num_epoch):
        print(f'Epoch {epoch+1}/{num_epoch}')
        print('-' * 40)

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            epoch_loss = 0.0
            epoch_corrects = 0

            # leave=False를 통해 이전 진행 내용을 삭제
            for inputs, labels in tqdm(dataloader_dict[phase], leave=False):
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()

                with torch.set_grad_enabled(phase=='train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        # 모든 파라미터의 기울기 계산
                        loss.backward()
                        # 파라미터 갱신
                        optimizer.step()
                    
                    epoch_loss += loss.item() * inputs.size(0)
                    epoch_corrects += torch.sum(preds == labels.data)
            
            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)
            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    
    return model

# 모델 학습
num_epoch = 10
model = train_model(model, dataloader_dict, criterion, optimizer, num_epoch)

# 모델을 이용한 예측

import pandas as pd

id_list, pred_list = [], []
_id = 0
with torch.no_grad():
    for test_path in tqdm(test_images_filepaths, leave=False):
        img = Image.open(test_path)
        _id = test_path.split('/')[-1].split('.')[1]
        transform = ImageTransform(size, mean, std)
        img = transform(img, phase='val')
        img = img.unsqueeze(0)
        img = img.to(device)

        model.eval()
        outputs = model(img)
        preds = F.softmax(outputs, dim=1)[:, 1].tolist()

        id_list.append(_id)
        # preds가 리스트 형태이기 때문에
        pred_list.append(preds[0])

res = pd.DataFrame({
    'id': id_list,
    'label': pred_list,
})
res.to_csv('./data/alexnet.csv', index=False)
# 예측 결과를 시각적으로 표현하기 위한 함수 정의

class_ = classes = {0: 'cat', 1: 'dog'}

def display_image_grid(images_filepaths, predicted_labels=(), cols=5):
    rows = len(images_filepaths) // cols
    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))
    for i, image_filepath in enumerate(images_filepaths):
        image = cv2.imread(image_filepath)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        a = random.choice(res['id'].values)
        label = res.loc[res['id'] == a, 'label'].values[0]
        if label > 0.5:
            label = 1
        else:
            label = 0
        
        ax.ravel()[i].imshow(image)
        ax.ravel()[i].set_title(class_[label])
        ax.ravel()[i].set_axis_off()
    
    plt.tight_layout()
    plt.show()

# 예측 결과에 대해 이미지와 함께 출력
display_image_grid(test_images_filepaths)
```
###### VGGNet
- 합성곱층의 파라미터 수를 줄이고 훈련 시간을 개선하려고 탄생
- 네트워크를 깊게 만드는 것이 성능에 어떤 영향을 미치는지 확인하고자 나온 것이 VGG
- 깊이의 영향만 최대한 확인하고자 합성곱층에서 사용하는 필터/커널의 크기를 가장 작은 3으로 했다는 것이 특징
- 모든 최대 풀링 커널의 2 x 2이며 스트라이드는 2, 결과적으로 64개의 224 x 224 특성 맵이 생성
- 마지막 16번째 계층을 제외하고 모두 ReLU 활성화 함수가 적용됨
- 네트워크 계층의 총 개수에 따라서 여러 유형의 VGG16, VGG19 등 VGGNet이 있음

얕은 복사는 값 변경은 원본에 적용되지 아니하나 값 추가는 공유하는 반면, 깊은 복사는 아예 별개의 데이터가 된다.  

`assert`: 가정 설정문, 뒤의 조건이 True가 아니면 에러를 발생  
`assert c == 'M'` c가 M이 아니면 에러를 발생시킴  
`isinstance`: 주어진 조건이 True인지 판단  
`instance(c, int)`: c가 int이면 True, 그렇지 않으면 False  

`batch_norm(Batch Normalization)`은 데이터의 평균을 0으로 표준편차를 1로 분포시키는 것  
`vgg19_bn`: 기존 모델을 배치 정규화가 적용된 모델을 사용하겠다.
`pretrained=True`: 사전 훈련된 모델을 사용하겠다.  
하지만, pretrained는 pytorch에서는 곧 사라질 것이다, 그러므로 weights 매개변수를 사용해라  

`transforms.Compose`: 데이터 전처리에 사용됨  
    - `transforms.Resize`: 이미지를 주어진 크기로 재조정
    - `transforms.RandomRotation`: 5도 이하로 이미지를 회전시킴

`correct = top_pred.eq(y.view_as(top_pred)).sum()`: 예측과 정답이 일치하는 경우, 그 개수의 합을 correct 변수에 할당
    - `eq`: equal, 서로 같은지 비교하는 표현식
    - `view_as(other)`: other의 텐서 크기를 사용하겠다 (=`view_as(other.size())`) -> y의 텐서 크기를 top_pred의 텐서 크기로 변경  

`argmax(1, keepdim=True)`: 배열에서 가장 큰 값의 인덱스를 찾을 때 사용  
    - 첫 번째 파라미터: 행(axis=0), 열(axis=1)을 따라 가장 큰 값의 색인을 찾음
    - keepdim: `keepdim=True`이라면 출력 텐서를 입력과 동일한 크기로 유지하겠음

`torch.cat`: 텐서를 연결할 때 사용
    - dim: 0이면 행, 1이면 열

``` python
import torch

x = torch.Tensor([[1, 2, 3], [2, 3, 4]])
y = torch.Tensor([[4, 5, 6], [5, 6, 7]])

# 행을 기준으로 x를 이어 붙임
print(torch.cat([x], dim=0))
print('------------')
# 단순하게 x와 y를 결합하라는 의미, 출력 결과는 (4 x 3)
print(torch.cat([x, y]))
print('------------')
# 행을 기준으로 x와 y를 이어붙이면 (4 x 3)
print(torch.cat([x, y], dim=0))
print('------------')
# 열을 기준으로 x와 y를 이어붙이면 (3 x 4)
print(torch.cat([x, y], dim=1))


"""
tensor([[1., 2., 3.],
        [2., 3., 4.]])
------------
tensor([[1., 2., 3.],
        [2., 3., 4.],
        [4., 5., 6.],
        [5., 6., 7.]])
------------
tensor([[1., 2., 3.],
        [2., 3., 4.],
        [4., 5., 6.],
        [5., 6., 7.]])
------------
tensor([[1., 2., 3., 4., 5., 6.],
        [2., 3., 4., 5., 6., 7.]])
------------
"""
```

max와 argmax의 차이: max는 값을, argmax는 인덱스를 반환한다.  

``` python
# (5 x 3) 형태의 형태 생성
x = torch.rand(5, 3)
print(x)

print(torch.max(x))
print('----------')
print(torch.argmax(x))

"""
tensor(0.9622)
---------
tensor(7)
"""
```

`torch.add`는 새로운 메모리 공간이 할당, 기존의 메모리에 위치한 값을 대체

``` python
# torch.add vs torch.add_

x = torch.tensor([1, 2])
# torch.add 적용
y = x.add(10)
print(y)
print(x is y)
print('-' * 10)
y = x.add_(10)
print(y)
print(x is y)

"""
tensor([11, 12])
False
----------
tensor([11, 12])
True
"""
```

`image.add_(-image_min)`: 이미지의 최소값을 0으로 맞추기 위해 사용, 각 픽셀 값이 양수가 됨  

`div_()`: pytorch의 텐서 메서드, 현재 텐서를 주어진 값으로 나눔

`image.permute`는 축을 변경할 때 사용
``` python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
print(x)
# 차원(축)과 0과 1을 바꿈 
x.permute(1, 0)

"""
tensor([[1, 2, 3],
        [4, 5, 6]])
tensor([[1, 4],
        [2, 5],
        [3, 6]])
"""
```
    
pytorch의 텐서는 기본적으로 [채널, 높이, 너비] 순서  
imshow는 [높이, 너비, 채널] 형식을 필요로 함  


###### GoogLeNet
주어진 하드웨어 자원을 최대한 효율적으로 이용하면서 학습 능력은 극대화 할 수 있는 깊고 넓은 신경망  
인셉션 모듈: 특징을 효율적으로 추출하기 위해 1 x 1, 3 x 3, 5 x 5의 합성곱 연산을 각각 수행함  

3 x 3 최대 풀링은 입/출력의 높이와 너비가 같아야 하므로 풀링 연산에서는 드물게 패딩을 추가해야 함  
> GoogLeNet에서는 희소 연결을 통해 해결했다.  

CNN에서는 합성곱, 풀링, 완전연결층이 서로 밀집되어 연결되어 있다.  

희소 연결: 빽빽하게 연결된 신경망 대신 관련성이 높은 노드끼리 연결하는 방법  
[인셉션 모듈의 네 가지 연산]  
- 1 x 1 합성곱
- 1 x 1 합성곱 + 3 x 3  합성곱
- 1 x 1 합성곱 + 5 x 5 합성곱
- 3 x 3 최대 풀링 + 1 x 1 합성곱

심층 신경망의 아키텍처에서 계층이 넓고(뉴런이 많고) 깊으면(계층이 많으면) 인식률은 좋아지지만, 과적합이나 기울기 소멸 문제를 비롯한 학습 시간의 문제가 존재  

###### ResNet
마이크로소프트에서 개발한 알고리즘, 깊어진 신경망을 효과적으로 학습하기 위한 방법으로 residual(레지듀얼)이라는 개념을 고안  

신경망은 깊이가 깊어질수록 성능이 좋아지다가 일정한 단계에 다다르면 오히려 성능이 나빠진다.  
> 네트워크의 깊이가 깊다고 성능이 좋아지는 것은 아님

레지듀얼 블록: 기울기가 잘 전파될 수 있게 일종의 숏컷을 만들어 줌  

블록: 계층의 묶음, 합성곱층을 하나의 블록으로 묶은 것, 그 하나의 블록은 레지듀얼 블록, 레지듀얼 블록을 여러 개 쌓은 것이 ResNet  

ResNet34는 합성곱층이 34개와 16개의 블록으로 구성되어 있음, 기본 블록을 사용 39.3216M  
ResNet50은 병목 블록을 사용함, 파라미터 수는 오히려 6.9632M  

아이덴티티 매핑(숏컷, 스킵 연결): 입력 x가 어떤 함수를 통과하더라도 다시 x라는 형태로 출력되도록 하는 것  
다운 샘플: 특성맵 크기를 줄이기 위한 것, 풀링과 같은 역할을 수행함  
아이덴티티는 각 블록 간의 형태를 같게 해야지 수행이 가능하다, 그래서 다운 샘플을 수행한다.  

아이덴티티 블록: 입력과 출력의 차원이 같은 것  
프로젝션 숏컷(합성곱 블록): 입력의 차원을 출력에 맞추어 변경해야 하는 것  

ResNet은 기본적으로 VGG19 구조를 뼈대로 하여 합성곱을 추가해서 깊게 만든 후, 숏컷을 추가하는 것  

`namedtuple` 튜플의 성질을 가지고 있는 자료형, 인덱스뿐만 아니라 키 값으로 데이터에 접근 가능  

``` python
from collections import namedtuple

# 네임드 튜플 정의
Student = namedtuple('Student', ['name', 'age', 'D0B'])
# 네임드 튜플에 값을 추가
S = Student('홍길동', '19', '187')

print('The Student age using index is : ', end='')
print(S[1])

print('The Student name using keyname is : ', end='')
print(S.name)

"""
The Student age using index is : 19
The Student name using keyname is : 홍길동
"""
```

기본 블록을 병목 블록으로 변경하는 이유는 계층을 깊게 쌓으면서 계산 비용을 줄일 수 있기 때문  
계층이 많아진다는 것은 활성화 함수가 많아짐, 비선형성을 더 잘 처리할 수 있음  
> 다양한 입력 데이터에 대한 처리 가능  

`nn.init.constant_`: 주어진 텐서를 특정 값으로 초기화하는 pytorch의 함수  

새로운 모델의 네트워크가 발표되면, 그것을 확인해서 구조부터 익히는 훈련을 하라

`tensor.topk`: `torch.argmax`와 같은 효과, 주어진 텐서에서 가장 큰 값의 인덱스를 얻기 위해 사용, 네트워크의 출력에서 가장 확률이 높은 인덱스 반환  

``` python
import torch

x = torch.arange(1., 6.)
print(x)
print('-' * 20)
# x 입력에서 가장 큰 값 3개를 선택하여 그 값과 인덱스를 출력
print(torch.topk(x, 3))

"""
tensor([1., 2., 3., 4., 5.])
--------------------
torch.return_types.topk(
values=tensor([5., 4., 3.]),
indices=tensor([4, 3, 2]))
"""
```

`top_pred= top_pred.t()`: `t()`은 차원 0과 1을 전치하겠다는 의미  

``` python
import torch

# 1차원 텐서
x = torch.randn(3)
print(x)
# 1차원 그대로 반환
print(torch.t(x))
print('-' * 20)
# (2, 3)차원 텐서 생성
x = torch.randn(2, 3)
print(x)
# 입력이 전치되어 (3, 2)차원 반환
print(torch.t(x))

"""
tensor([ 0.7858, -0.1210, -0.2056])
tensor([ 0.7858, -0.1210, -0.2056])
--------------------
tensor([[ 0.0409,  0.1517, -0.8562],
        [-1.2836, -0.8919,  0.2773]])
tensor([[ 0.0409, -1.2836],
        [ 0.1517, -0.8919],
        [-0.8562,  0.2773]])
"""
```

`torch.eq()`: 두 텐서가 서로 같은지 비교한다면  
`torch.ne()`: 두 텐서가 서로 다른지를 비교한다면  
`torch.ge()`: 두 텐서가 크거나 같은지를 비교한다면  

``` python
import torch

torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))
"""
tensor([[ True, False],
        [False,  True]])
"""
```

`correct[:1].reshape(-1)`: 텐서를 1D 텐서로 변경, 여기서 인자 -1은 pytorch에 해당 차원의 크기를 자동으로 계산하라고 지시하는 역할  
> reshape(-1)는 (k * batch_size, ) 형태의 1D 텐서로 변경됨  

``` python
# 필요한 라이브러리 호출
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models

import matplotlib.pyplot as plt
import numpy as np

import copy
from collections import namedtuple
from tqdm import trange
import os
import random
import time

import cv2
from torch.utils.data import DataLoader, Dataset
from PIL import Image

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 이미지 데이터 전처리
class ImageTransform():
    
    def __init__(self, resize, mean, std):
        self.data_transform = {
            # 훈련 이미지 데이터에 대한 전처리
            'train': transforms.Compose([
                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(mean, std),
            ]),
            # 검증과 테스트 이미지 데이터에 대한 전처리
            'val': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(resize),
                transforms.ToTensor(),
                transforms.Normalize(mean, std),
            ])
        }
    
    def __call__(self, img, phase):
        return self.data_transform[phase](img)

# 변수에 대한 값 정의
size = 224
mean = (0.485, 0.456, 0.406)
std = (0.229, 0.224, 0.225)
batch_size = 32

# 훈련과 테스트 데이터셋 불러오기
cat_directory = r'./data/dogs-vs-cats/Cat/' # r-string을 통해 문자열 내의 백슬래시를 그대로 사용할 수 있다.
dog_directory = r'./data/dogs-vs-cats/Dog/'
test_directory = r'./data/dogs-vs-cats/Test/'

cat_images_filepaths = sorted(os.path.join(cat_directory, f) for f in os.listdir(cat_directory))
dog_images_filepaths = sorted(os.path.join(dog_directory, f) for f in os.listdir(dog_directory))
test_images_filepaths = sorted(os.path.join(test_directory, f) for f in os.listdir(test_directory))

images_filepaths = [*cat_images_filepaths, *dog_images_filepaths]
# 손상파일, 다른 파일 형식, 잘못된 경로 등을 걸러내기 위한 코드
correct_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None]

random.seed(42)
random.shuffle(correct_images_filepaths)

# 데이터셋을 훈련, 검증, 테스트 용도로 분리
train_images_filepaths = correct_images_filepaths[:20000]
val_images_filepaths = correct_images_filepaths[20000:-10]
test_images_filepaths = [*test_images_filepaths, *correct_images_filepaths[-10:]]

print(len(train_images_filepaths), len(val_images_filepaths), len(test_images_filepaths))

# 이미지에 대한 레이블 구분
class DogvsCatDataset(Dataset):

    def __init__(self, file_list, transform=None, phase='train'):
        self.file_list = file_list
        # 채널이 3개가 아닌 이미지를 필터링하여 새로운 리스트에 저장
        self.file_list = [f for f in self.file_list if Image.open(f).mode == 'RGB']
        self.transform = transform
        self.phase = phase
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        img_path = self.file_list[idx]
        img = Image.open(img_path)
        img_transformed = self.transform(img, self.phase)

        label = img_path.split('/')[-1].split('.')[0]
        if label == 'dog':
            label = 1
        elif label == 'cat':
            label = 0
        return img_transformed, label

# 이미지 데이터셋 정의
train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')
val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')

index = 0
print(train_dataset.__getitem__(index)[0].size())
print(val_dataset.__getitem__(index)[1])

# 데이터셋의 데이터를 메모리로 불러오기
train_iterator = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_iterator = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
dataloader_dict = {
    'train': train_iterator,
    'val': valid_iterator,
}

batch_iterator = iter(train_iterator)
inputs, label = next(batch_iterator)
print(inputs.size())
print(label)

# 기본 블록 정의
class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, downsample=False):
        super().__init__()
        # 3 x 3 합성곱층
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        # 3 x 3 합성곱층
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

        if downsample:
            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)
            bn = nn.BatchNorm2d(out_channels)
            downsample = nn.Sequential(conv, bn)
        else:
            downsample = None
        # 다운 샘플링이 필요한 경우, downsample의 값을 객체에 저장
        self.downsample = downsample
    
    def forward(self, x):
        i = x
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)

        # 다운샘플이 필요한 경우, 원래 입력 i에 다운샘플을 적용
        if self.downsample is not None:
            i = self.downsample(i)
        
        x += i
        x =  self.relu(x)
        
        return x

# 병목 블록 정의
class Bottleneck(nn.Module):
    # ResNet에서 병목 블록을 정의하기 위한 하이퍼파라미터
    expansion = 4

    def __init__(self, in_channels, out_channels, stride=1, downsample=False):
        super().__init__()
        # 1 x 1 합성곱층
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        # 3 x 3 합성곱층
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        # 1 x 1 합성곱층, 다음 계층 채널 수와 일치하도록 self.expansion x out_channels를 적용
        self.conv3 = nn.Conv2d(out_channels, self.expansion*out_channels, kernel_size=1, stride=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*out_channels)
        self.relu = nn.ReLU(inplace=True)

        if downsample:
            conv = nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1, stride=stride, bias=False)
            bn = nn.BatchNorm2d(self.expansion*out_channels)
            # 합성곱 레어어와 배치 정규화를 하나의 모듈로 묶어준다.
            downsample = nn.Sequential(conv, bn)
        else:
            downsample = None
        self.downsample = downsample

    def forward(self, x):
        i = x
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)

        if self.downsample is not None:
            i = self.downsample(i)
        
        x += i
        x = self.relu(x)
        return x

# ResNet 모델 네트워크
class ResNet(nn.Module):
    
    def __init__(self, config, output_dim, zero_init_residual=False):
        super().__init__()

        # Resnet을 호출할 때 넘겨준 config 값을 block, n_blocks, channels에 저장
        block, n_blocks, channels = config
        self.in_channels = channels[0]
        # 블록 크기 = 채널 크기 = 4가 아니면 오류 발생
        # ResNet의 표준 구조는 레이어를 4개 사용하기 때문에 이를 강제함
        assert len(n_blocks) == len(channels) == 4
        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)
        # self.in_channels는 출력 채널 수
        self.bn1 = nn.BatchNorm2d(self.in_channels)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.layer1 = self.get_resnet_layers(block, n_blocks[0], channels[0])
        # ResNet에서 다운 샘플링 시, stride는 2로 한다.
        self.layer2 = self.get_resnet_layers(block, n_blocks[1], channels[1], stride=2)
        self.layer3 = self.get_resnet_layers(block, n_blocks[2], channels[2], stride=2)
        self.layer4 = self.get_resnet_layers(block, n_blocks[3], channels[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(self.in_channels, output_dim)
        
        if zero_init_residual:
            # 모델의 모든 모듈과 서브 모듈(합성곱, 배치 정규화 등을 포함)
            for m in self.modules():
                # 잔차 연결의 초기화를 돕기 위해 배치 정규화 레이어의 가중치를 0으로 초기화
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)
    
    # 블록을 추가하기 위한 함수
    def get_resnet_layers(self, block, n_blocks, channels, stride=1):
        layers = []
        # in_channels와 block.exapnsion * channels가 다르면 dowmsample 적용
        if self.in_channels != block.expansion * channels:
            downsample = True
        else:
            downsample = False

        # 계층을 추가할 때, in_channels, channels, stride 뿐만 아니라 다운 샘플 적용 유무도 함께 전달    
        layers.append(block(self.in_channels, channels, stride, downsample))
        # n_blocks만큼 계층 추가
        for i in range(1, n_blocks):
            layers.append(block(block.expansion*channels, channels))

        self.in_channels = block.expansion * channels
        # 여러 개의 레이어 또는 모듈을 순차적으로 묶어 하나의 모듈로 만드는 데 사용
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x) # 224 x 224
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x) # 112 x 112
        x = self.layer1(x) # 56 x 56
        x = self.layer2(x) # 28 x 28
        x = self.layer3(x) # 14 x 14
        x = self.layer4(x) # 7 x 7
        x = self.avgpool(x) # 1 x 1
        h = x.view(x.shape[0], -1)
        x = self.fc(h)
        return x, h

# ResNetConfig 정의
ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])

# 기본 블록을 사용하여 ResNetConfig 정의
resnet18_config = ResNetConfig(block=BasicBlock,
                               n_blocks=[2, 2, 2, 2],
                               channels=[64, 128, 256, 512])

resnet34_config = ResNetConfig(block=BasicBlock,
                               n_blocks=[3, 4, 6, 3],
                               channels=[64, 128, 256, 512])
                               
# 병목 블록을 사용하여 ResNetConfig 정의
resnet50_config = ResNetConfig(block=Bottleneck,
                               n_blocks=[3, 4, 6, 3],
                               channels=[64, 128, 256, 512])

resnet101_config = ResNetConfig(block=Bottleneck,
                               n_blocks=[3, 4, 23, 3],
                               channels=[64, 128, 256, 512])

resnet152_config = ResNetConfig(block=Bottleneck,
                               n_blocks=[3, 8, 36, 3],
                               channels=[64, 128, 256, 512])

# 사전 훈련된 ResNet 모델 사용
pretrained_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

print(pretrained_model)

# ResNet50 Config를 사용한 ResNet 모델 사용
# 2개의 클래스 사용(개와 고양이)
OUTPUT_DIM = 2
model = ResNet(resnet50_config, OUTPUT_DIM)
print(model)

# 옵티마이저와 손실 함수 정의
optimizer = optim.Adam(model.parameters(), lr=1e-7)
criterion = nn.CrossEntropyLoss()

model = model.to(device)
criterion = criterion.to(device)

# 모델 학습 정확도 측정 함수 정의
def calculate_topk_accuracy(y_pred, y, k=2):
    with torch.no_grad():
        batch_size = y.shape[0]
        # 상위 2개의 예측 값을 가져 옴
        _, top_pred = y_pred.topk(k, 1)
        # k, batch_size 형태로 만듦
        top_pred = top_pred.t()
        # 실제 레이블과 예측값을 동일한 크기로 확장
        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))
        # 첫 번째 예측값이 맞는지 확인
        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim=True)
        # 이미지의 정확한 레이블 부여를 위해 사용, 즉, 첫 번째 레이블이 아닌 정확한 레이블 부여를 위해 사용
        # 상위 k개의 예측 값 중 하나라도 맞는지 확인, 예측과 실제 레이블을 요소 별로 비교하기 위해 입/출력 텐서를 같게 유지
        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
        acc_1 = correct_1 / batch_size
        acc_k = correct_k / batch_size
    return acc_1, acc_k

# 모델 학습 함수 정의
def train(model, iterator, optimizer, criterion, device):
    epoch_loss, epoch_acc_1, epoch_acc_5 = 0, 0, 0
    model.train()
    
    for (x, y) in iterator:
        x = x.to(device)
        y = y.to(device)

        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred[0], y)

        acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        # 모델이 첫 번째로 예측한 레이블이 붙여짐
        epoch_acc_1 += acc_1.item()
        # 이미지에 정확한 레이블이 붙여질 것이기 때문에 정확도가 100%이다.
        epoch_acc_5 += acc_5.item()
    
    epoch_loss /= len(iterator)
    epoch_acc_1 /= len(iterator)
    epoch_acc_5 /= len(iterator)
    return epoch_loss, epoch_acc_1, epoch_acc_5

# 모델 평가 함수 정의
def evaluate(model, iterator, criterion, device):
    epoch_loss, epoch_acc_1, epoch_acc_5 = 0, 0, 0
    model.eval()

    with torch.no_grad():
        for (x, y) in iterator:
            x = x.to(device)
            y = y.to(device)
            y_pred = model(x)
            loss = criterion(y_pred[0], y)

            acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)
            epoch_loss += loss.item()
            epoch_acc_1 += acc_1.item()
            epoch_acc_5 += acc_5.item()

        epoch_loss /= len(iterator)
        epoch_acc_1 /= len(iterator)
        epoch_acc_5 /= len(iterator)
        return epoch_loss, epoch_acc_1, epoch_acc_5

# 모델 학습 시간 측정 함수 정의
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapse_mins = int(elapsed_time / 60)
    elapse_secs = int(elapsed_time - (elapse_mins * 60))
    return elapse_mins, elapse_secs

# 모델 학습
best_valid_loss = float('inf')
EPOCHS = 10

for epoch in trange(EPOCHS, leave=False):
    start_time = time.monotonic()

    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer, criterion, device)
    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)

    if best_valid_loss > valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), './data/ResNet-model.pt')
    
    end_time = time.monotonic()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)

    # 폭을 2자리로 하되, 0으로 2자리를 채워라 1 -> 01
    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    # 정확도는 전체 폭을 6으로 한다. '85.12' -> ' 85.12' 
    print(f'\t Train Loss: {train_loss:.3f} | Train Acc: @1: {train_acc_1 * 100:6.2f}% | ' \
          f'Train Acc @5: {train_acc_5 * 100:6.2f}%')
    print(f'\t Valid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1 * 100:6.2f}% | ' \
          f'Valid Acc @5: {valid_acc_5 * 100:6.2f}%')

# 테스트 데이터셋을 이용한 모델 예측
import pandas as pd
id_list, pred_list = [], []
_id = 0

with torch.no_grad():
    for test_path in test_images_filepaths:
        img = Image.open(test_path)
        _id = test_path.split('/')[-1].split('.')[1]
        transform = ImageTransform(size, mean, std)
        img = transform(img, 'val')
        img = img.unsqueeze(0)
        img = img.to(device)

        model.eval()
        outputs = model(img)
        # [batch_size, num_classes 형태의 텐서로 반환하고, num_classes에 대한 확률 선택]
        preds = F.softmax(outputs[0], dim=1)[:, 1].tolist()
        id_list.append(_id)
        pred_list.append(preds[0])
    
res = pd.DataFrame({
    'id': id_list,
    'label': pred_list,
})

# res.sort_values(by='id', inplace=True)
res.reset_index(drop=True, inplace=True)

res.to_csv('./data/ResNet.csv', index=False)
res.head(20)

# 모델 예측에 대한 결과 출력

class_ = classes = {0: 'cat', 1: 'dog'}

def display_image_grid(images_filepaths, predicted_labels=(), cols=5):
    rows = len(images_filepaths) // cols
    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))
    figure.suptitle('ResNet')
    for i, images_filepath in enumerate(images_filepaths):
        image = cv2.imread(images_filepath)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        id = res.loc[i, 'id']
        label = res.loc[i, 'label']
        # 원재 저장된 label을 보존
        _original_label = label

        if label > 0.5:
            label = 1
        else:
            label = 0
        
        ax.ravel()[i].imshow(image)
        ax.ravel()[i].set_title(f' {class_[label]}', fontsize=8)
        ax.ravel()[i].text(1.5, 0, f'{id} \n {_original_label:.6f}', fontsize=6, ha='center', transform=ax.ravel()[i].transAxes)
        ax.ravel()[i].set_axis_off()

    plt.tight_layout()
    plt.show()

display_image_grid(test_images_filepaths)

"""
 10%|████████▏                                                                         | 1/10 [04:48<43:20, 288.89s/it]
Epoch: 01 | Epoch Time: 4m 48s
	 Train Loss: 0.684 | Train Acc: @1:  56.72% | Train Acc @5: 100.00%
	 Valid Loss: 0.681 | Valid Acc @1:  57.32% | Valid Acc @5: 100.00%
 20%|████████████████▍                                                                 | 2/10 [13:59<59:01, 442.64s/it]
Epoch: 02 | Epoch Time: 9m 10s
	 Train Loss: 0.679 | Train Acc: @1:  57.64% | Train Acc @5: 100.00%
	 Valid Loss: 0.676 | Valid Acc @1:  58.57% | Valid Acc @5: 100.00%
 30%|████████████████████████                                                        | 3/10 [29:43<1:18:23, 671.86s/it]
Epoch: 03 | Epoch Time: 15m 44s
	 Train Loss: 0.676 | Train Acc: @1:  58.14% | Train Acc @5: 100.00%
	 Valid Loss: 0.672 | Valid Acc @1:  59.07% | Valid Acc @5: 100.00%
 40%|████████████████████████████████                                                | 4/10 [45:33<1:18:08, 781.42s/it]
Epoch: 04 | Epoch Time: 15m 49s
	 Train Loss: 0.674 | Train Acc: @1:  58.46% | Train Acc @5: 100.00%
	 Valid Loss: 0.669 | Valid Acc @1:  59.74% | Valid Acc @5: 100.00%
 50%|███████████████████████████████████████                                       | 5/10 [1:01:16<1:09:59, 839.93s/it]
Epoch: 05 | Epoch Time: 15m 43s
	 Train Loss: 0.672 | Train Acc: @1:  58.90% | Train Acc @5: 100.00%
	 Valid Loss: 0.667 | Valid Acc @1:  59.83% | Valid Acc @5: 100.00%
 60%|████████████████████████████████████████████████                                | 6/10 [1:17:01<58:22, 875.61s/it]
Epoch: 06 | Epoch Time: 15m 44s
	 Train Loss: 0.671 | Train Acc: @1:  59.05% | Train Acc @5: 100.00%
	 Valid Loss: 0.667 | Valid Acc @1:  59.64% | Valid Acc @5: 100.00%
 70%|████████████████████████████████████████████████████████                        | 7/10 [1:32:50<44:58, 899.44s/it]
Epoch: 07 | Epoch Time: 15m 48s
	 Train Loss: 0.669 | Train Acc: @1:  59.26% | Train Acc @5: 100.00%
	 Valid Loss: 0.664 | Valid Acc @1:  60.98% | Valid Acc @5: 100.00%
 80%|████████████████████████████████████████████████████████████████                | 8/10 [1:48:30<30:24, 912.45s/it]
Epoch: 08 | Epoch Time: 15m 40s
	 Train Loss: 0.668 | Train Acc: @1:  59.43% | Train Acc @5: 100.00%
	 Valid Loss: 0.661 | Valid Acc @1:  61.50% | Valid Acc @5: 100.00%
 90%|████████████████████████████████████████████████████████████████████████        | 9/10 [2:04:16<15:22, 922.95s/it]
Epoch: 09 | Epoch Time: 15m 46s
	 Train Loss: 0.667 | Train Acc: @1:  59.99% | Train Acc @5: 100.00%
	 Valid Loss: 0.662 | Valid Acc @1:  61.29% | Valid Acc @5: 100.00%
                                                                                                                       
Epoch: 10 | Epoch Time: 15m 42s
	 Train Loss: 0.666 | Train Acc: @1:  59.80% | Train Acc @5: 100.00%
	 Valid Loss: 0.660 | Valid Acc @1:  61.72% | Valid Acc @5: 100.00%
"""
```
800 -> 25000개로 데이터셋 늘렸는데도 정확도가 10% 정도 밖에 안 올라갔다.  
epoch나 batch_size를 늘리면 될 수도 있겠지만 gpu 메모리 터질까봐 함부로 못 올리겠다.  

##### 객체 인식을 위한 신경망
객체 인식은 이미지나 영상 내에 있는 객체를 식별하는 컴퓨터 비전 기술  
- 이미지나 영상 내에 있는 여러 객체에 대해 각 객체가 무엇인지 분류하는 문제
- 그 객체 위치가 어디인지 박스로 나타내는 위치 검출 문제

- 1단계 객체 인식(1-stage-detector): 분류와 위치 검출을 동시에 수행(YOLO, SSD 계열)  
- 2단게 객체 인식(2-stage-detector): 분류와 위치 검출을 순차적으로 수행(R-CNN 계열)  

###### R-CNN
예전의 객체 인식 알고리즘: 슬라이딩 윈도우 방식, 일정한 크기를 가지는 윈도우를 가지고 이미지의 모든 영역을 탐색하면서 객체를 검출해 내는 방식  
알고리즘의 비효율성으로 인해 최근에는 선택적 탐색 알고리즘을 적용한 후보 영역을 많이 사용함  

R-CNN(region-based cnn): 이미지 분류를 수행하는 CNN + 이미지에서 객체가 있을 만한 영역을 제안하는 후보 영역 알고리즘을 결합  

[절차]  
1. 이미지를 입력으로 받음
2. 2000개의 바운딩 박스를 선택적 탐색 알고리즘으로 추출한 후 잘라 내고, cnn 모델에 넣기 위해 같은 크기 (227 x 227)로 통일
3. 크기가 동일한 이미지 2000개에 각각 CNN 모델을 적용합니다.
4. 각각 분류를 진행하여 결과를 도출합니다.

선택적 탐색: 객체 인식이나 검출을 위한 가능한 후보 영역을 알아내는 방법, 분할 방식을 이용하여 시드를 선정하고, 시드에 대한 완전 탐색을 진행  
1. 초기 영역 생성: 각각의 객체가 한 개에 할당될 수 있도록 많은 초기 영역을 생성, 입력된 이미지를 영역 다수 개로 분할하는 과정
2. 작은 영역의 통합: 영역 여러 개를 비슷한 영역으로 탐욕 알고리즘을 통해 비슷한 영역이 하나로 통합될 때까지 반복
3. 후보 영역 생성: 통합된 이미지를 기반으로 후보 영역(바운딩 박스)을 추출

* 시드: 영상에 특정 기준점의 픽셀에서 점점 의미가 같은 영상 범위까지 픽셀을 확장해 나가며 분할하는데 이 때 특정 기준점이 되는 픽셀

[R-CNN의 한계]  
1. 세 단계의 복잡한 학습 과정
2. 1번으로 인한 긴 학습 시간과 대용량 저장 공간
3. 객체 검출 속도 문제

###### 공간 피라미드 풀링
기존 CNN 구조는 모두 완전연결층을 위해 입력 이미지를 고정해야 했다.  
그래서 신경망을 통과시키려면 이미지를 고정된 크기로 자르거나(Crop) 비율을 조정(Warp)해야 했음  

하지만, 사물의 일부분이 잘리거나 본래의 생김새와 달라지는 문제점 발생  

> 이러한 문제를 해결하고자 **공간 피라미드 풀링**을 도입  

합성곱층 -> 최대 풀링 -> 연결하기 -> 완전연결층 -> 바운딩 박스, SVM(분류기)  

공간 피라미드 풀링은 입력 이미지의 크기에 상관없이 합성곱층을 통과시키고, 완전연결층에 전달되기 전에 특성 맵을 동일한 크기로 조절해 주는 풀링층을 적용하는 기법  
- 입력 이미지의 크기를 조절하지 않고, 합성곱층을 통과하므로 원본 이미지의 특징이 훼손되지 않는 특성맵을 얻을 수 있음
- 이미지 분류나 객체 인식 등 여러 분야에서 활용될 수 있음

###### Fast R-CNN
R-CNN은 바운딩 박스마다 CNN을 돌리고, 분류를 위한 긴 학습 시간이 문제  
Fast R-CNN은 이 문제를 해결하고자 Rol 풀링을 도입  

선택적 탐색에서 찾은 바운딩 박스 정보가 CNN을 통과하면서 유지되도록 하고 최종 CNN 특성 맵은 풀링을 적용하여 완전연결층을 통과하도록 크기를 조정합니다.  
> 바운딩 박스마다 CNN을 돌리는 시간을 단축 가능

Rol 풀링: 크기가 다른 특성 맵의 영역 마다 스트라이드를 다르게 최대 풀링을 적용하여 결괏값 크기를 동일하게 맞추는 방법  

가령, 8 x 8 특성맵에서 선택적 탐색으로 7 x 5 영역을 뽑아냈다면 2 x 2로 만들기 위해서 스트라이드(7/2=3, 5/2=2)로 풀링 영역을 정하고 최대 풀링을 적용하면 2 x 2 영역을 얻을 수 있고, 그 안에서 최대 값을 선정해 2 x 2 크기의 피처맵을 얻을 수 있다. 

###### Faster R-CNN
더욱 빠른 객체 인식을 수행하기 위한 네트워크  

기존 Fast R-CNN 속도의 걸림돌이었던 후보 영역 생성을 CNN 내부 네트워크에서 진행할 수 있도록 설계  
> Fast R-CNN에 후보 영역 추출 네트워크를 추가한 것이 핵심  

RPN(GPU로 계산)을 사용하는 것도 핵심(선택적 탐색 방식과는 차이가 있다.)   

RPN은 마지막 합성곱층 다음에 위치하고, 그 뒤로는 Fast R-CNN과 마찬가지로 RoI풀링과 분류기, 바운딩 박스 회귀가 위치함  

- 후보 영역 추출 네트워크는 특성 맵 N x N 크기의 작은 윈도우 영역을 입력으로 받고, 해당 영역에 객체의 존재 유무 판단을 위해 이진 분류를 수행하는 작은 네트워크를 생성, 바운딩 박스 회귀 또한 위치 보정을 위해 추가
- 하나의 특성 맵에서 모든 영역에 대한 객체의 존재 유무를 확인하기 위해서 슬라이딩 윈도우 방식으로 앞서 설계한 작은 윈도우 영역을 이용하여 객체를 탐색

하지만, 이미지의 객체 크기와 비율이 다양하기에 고정된 N x N 크기의 입력만으로 다양한 크기와 비율의 이미지를 수용하기 어려운 한계 존재  
이를 보완하기 위해 여러 크기와 비율의 레퍼런스 박스 k개를 미리 정의하고 각각의 슬라이딩 윈도우마다 박스 k개를 출력하도록 설계(앵커)  
> 후보 영역 추출 네트워크의 출력 값은 모든 앵커 위치에 대해 각각 객체와 배경을 판단하는 2k개의 분류에 대한 출력과 x, y, w, h 위치 보정 값을 위한 4k개의 회귀 출력을 갖는다.  

ex. 맵 크기가 w x h 라면 하나의 특성 맵에 앵커가 w x h x k가 존재함  

##### 이미지 분할을 위한 신경망
이미지 분할은 신경망은 훈련시켜 이미지를 픽셀 단위로 분할하는 것  
> 이미지를 픽셀 단위로 분할하여 이미지에 포함한 객체를 추출  

###### 완전 합성곱 네트워크
완전연결층의 단점은 고정된 크기의 입력만 받아들이며 완전연결층을 거친 후에는 위치 정보가 사라진다는 것  
> 완전 연결층을 1 x 1 합성곱으로 대체하는 것

완전 합성곱 네트워크(FCN)은 이미지 분류에서 우수한 성능을 보인 CNN 기반 모델(AlexNet, VGG16, GoogLeNet)을 변형시켜 이미지 분할에 적합하도록 만든 네트워크  

[한계]  
- 여러 단계의 합성곱층과 풀링층을 거치면서 해상도가 낮아짐
- 낮아진 해상도를 복원하기 위해 업 샘플링 방식을 사용하므로, 이미지의 세부 정보를 잃어버리는 문제 발생

###### 합성곱 & 역합성곱 네트워크
완전 합성곱 네트워크의 한게를 극복하고자 역합성곱 네트워크를 도입한 것  

역합성곱은 CNN의 최종 출력 결과를 원래의 입력 이미지와 같은 크기로 만들고 싶을 때 사용  
시멘틱 분할 등에 사용할 수 있으며, 업 샘플링이라고도 함  

쉽게 말해 CNN에서는 합성곱을 사용하여 특성 맵 크기를 줄이는데, 역합성곱은 이와 반대로 특성맵 크기를 증가하는 방식으로 동작  

[동작 방식]  
1. 각각의 픽셀 주위로 제로 패딩을 추가
2. 패딩된 것에 합성곱 연산을 수행

0의 값을 가진 패딩을 주변에 추가해서 합성곱을 수행해서 너비를 넓히는 것이군

###### U-Net
바이오 메디컬 이미지 분할을 위한 합성곱 신경망  

패치: 이미지 인식 단위  

[특징]  
- 속도가 빠름: 기존의 슬라이딩 윈도우 방식은 이전 패치에서 검증이 끝난 부분도 다음 패치에서 또 검증하기 때문에 비효율적 -> 허나 U-Net은 그런 것은 과감히 생략
- 트레이드오프에 빠지지 않음: 패치 크기가 커지면 넓은 범위의 이미지를 인식하는 데 우수해 컨텍스트 인식에 탁월, 허나 지역화에는 한계가 존재, 하지만,  U-Net은 컨텍스트 인식과 지역화 트레이드 오프 문제를 개선함  

[구조]
- FCN을 기반으로 구축, 수축 경로와 확장 경로로 구성
    * 수축 경로는 컨텍스트를 포착
    * 확장 경로는 특성 맵을 업 샘플링
- 3 x 3 합성곱이 주를 이룸, 각 합성곱 블록은 3 x 3 합성곱 두 개로 구성, 그 사이에 드롭아웃 존재
- 수축 경로에서는 3 x 3 합성곱 두 개로 구성된 것이 4개가 존재, 그리고 각 블록 사이에 최대 풀링을 적용하여 크기를 줄여가며 뻗어 나감
- 확장 경로에서는 합성곱 블록에 up-conv라는 것을 앞에 붙여서 수축 과정에서 줄어든 크기를 다시 키워 감
> 크기가 다양한 이미지의 객체를 분할 하기 위해 크기가 다양한 특성 맵을 병합할 수 있도록 다운 샘플링과 업 샘플링을 순서대로 반복하는 구조  

###### PSPNet
CVPR 2017에서 발표된 시멘틱 분할 알고리즘  

완전연결층의 한계를 극복하기 위해 피라미드 풀링 모듈을 추가

[훈련 과정]  
1. 이미지 출력이 서로 다른 크기가 되도록 여러 차례 풀링을 수행
> 1x1, 2x2, 3x3, 6x6 크기로 풀링을 수행, 1x1 크기의 특성맵은 가장 광범위한 정보를 담고, 각각 다른 크기의 특성 맵은 서로 다른 영역의 정보를 담음
2. 1x1 합성곱을 사용하여 채널 수를 조정, 풀링층 개수를 N이라고 할 때, `출력 채널 수 = 입력 채널 수 / N`
3. 모듈의 입력 크기에 맞게 특성 맵을 업 샘플링(병합하기 위해서는 원본 크기로 바꿔야 하므로), 양선형 보간법이 사용
4. 원래의 특성 맵과 1~3 과정에서 생성한 새로운 특성 맵을 병합함

보간법: 화소 값을 할당받지 못한 영상(영상의 빈 공간)의 품질은 안 좋을 수 밖에 없는데 이 때 빈 화소에 값을 할당하여 좋은 품질의 영상을 만드는 방법   
    * 선형 보간법(linear interpolation): 원시 영상의 화소 값 2개를 사용하여 원하는 좌표에서 새로운 화소 값을 계산하는 방법  
    * 양선형 보간법(bilinear interpolation): 2차원 공간에서의 선형 보간법(3번)으로 네 개의 인접한 픽셀값을 사용하여 가중치를 곱하여 새로운 픽셀 값을 계산  

###### DeepLabv3/DeepLabv3+
완전 연결층의 단점을 보완하기 위해 Atrous 합성곱을 사용하는 네트워크  

인코더와 디코더 구조를 가지며, 일반적으로 인코더-디코더 구조에서 불가능했던 인코더에서 추출된 특성 맵의 해상도를 Atrous 합성곱을 도입하여 제어할 수 있도록 함  

Atrous 합성곱은 필터 내부에 빈 공간을 둔 채로 작동  
rate를 통해 조정하는데 rate=1이면 기존의 합성곱과 동일한 빈공간을 가짐, r이 커질수록 빈 공간은 많아짐  

이미지 분할에서 높은 성능을 내려면 수용 영역을 확대하여 특성을 찾는 범위를 넓게 해줘야 함  
Atrous 합성곱을 활용하면 파라미터 수를 늘리지 않으면서도 수용 영역을 크게 키울 수 있기에 이미지 분할 분야에서 많이 사용함  
CNN은 출력은 입력 대비 1/32 줄어들지만, Atrous 합성곱을 1/8로 줄어듦  