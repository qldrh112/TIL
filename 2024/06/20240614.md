# 20240614
## AI 드론봇 - 12


#### 강화 학습

##### 강화 학습이란
머신 러닝/딥러닝의 한 종류로 어떤 환경에서 어떤 행동을 했을 때, 그것이 잘 된 행동인지 아닌지 판단하고, 보상 혹은 벌칙을 주는 과정을 반복하여 스스로 학습하게 하는 분야  

환경: 에이전트가 다양한 행동을 해 보고, 그에 따른 결과를 관측할 수 있는 시뮬레이터를 가리킴  
에이전트: 환경에서 행동하는 주체  
  ex. 게임에서는 게임기가 환경, 게이머가 에이전트  
상태: 에이전트가 관찰 가능한 상태의 집합  
행동: 에이전트가 상태 S가 가능한 행동, 전체 행동 집합을 A, 시간은 t,  

강화 학습 문제는 마르코프 결정 과정으로 표현, 이 과정은 모두 마르코프 프로세스에 기반  

##### 마르코프 결정 과정
강화 학습 = 마르코프 결정 과정 + 학습 개념  

###### 마르코프 프로세스
어떤 상태가 일정한 간격으로 변하고, 다음 상태는 현재 상태에만 의존하는 확률적 상태 변화를 의미(MP라는 약어 존재)  
> 현재 상태에 대해서만 다음 상태가 결정, 현재 상태까지의 과정은 고려할 필요가 없다.  
변화 상태가 체인처럼 엮여 있다고 하여 마르코프 체인이라고도 함  

또 다른 정의: 마르코프 특성을 지니는 이산 시간에 대한 확률 과정  

  확률 과정: 시간에 따라 어떤 사건의 확률이 변화하는 과정  
  이산 시간: 시간이 연속적이 아닌 이산적으로 변함을 의미  
  마르코프 특성: 과거 상태와 현재 상태가 주어졌을 때, 미래 상태는 과거 상태와 독립적으로 현재 상태로만 결정된다.  

마르코프 체인: 시간에 따른 상태 변화를 나타냄, 상태 변화를 전이라고 함  
  전이는 확률로 표현 가능(전이 확률)  
  상태 전이 확률은 어떤 상태 i가 있을 때, 그 다음 상태 j가 될 확률  
  하나의 상태에서 다른 상태로 이동할 확률의 합은 1을 유지한다.  

전이 확률 행렬: 상태 전이 확률을 행렬 형태로 표현한 것  

주의 사항: 현재 상태가 바로 이전의 상태로 결정된다?  
세밀하게 표현하면 현재 상태는 이전 상태에 의존한다.  
> 이전에 많은 상태가 있더라도 다음 상태에 영향을 미치는 것은 현재 상태  
현재 상태에서 다음 상태를 결정하기 위해서는 여러 가지 확률 변수가 개입  

###### 마르코프 보상 프로세스
마르코프 프로세스에서 각 상태마다 좋고 나쁨이 추가된 확률 모델  

리턴(return): 각 상태의 보상 총합  
상태의 정확한 가치는 어느 시점에서 보상을 받는가?  
특성 상태에 도달하면 즉시 보상을 받나? 아니면 나중에 받나?  
나중에 받으면 미래 가치를 현재 가치로 변환해야 한다.  
이거 현재 가치와 미래 가치 회계학의 그것을 그대로 차용하는 것 같은데?  

할인율인 적용된 리턴은 G로 표시함  
가치 함수: 현재 상태가 s일 때, 앞으로 발생할 것으로 기대되는 모든 보상의 합을 가치라고 함  
> 현재 시점에서 미래의 모든 기대되는 보상을 표현하는 미래 가치  
강화 학습의 핵심: 가치 함수를 최대한 정확하게 찾는 것, 미래 가치가 가장 클 것으로 기대되는 결정하고 행동하는 것  

현재 무언가의 가치는 미래 얻을 보상을 현재 가치로 환산한 것이네  

###### 마르코프 결정 과정
기존 마르코프 보상 과정에서 행동이 추가된 확률 모델  

MDP(Markov Decision Process) 목표는 정의된 문제에 대해 각 상태마다 전체적인 보상을 최대화하는 행동이 무엇인지 결정  
  정책: 각각의 상태마다 행동 분포를 표현하는 함수
  상태-가치 함수: 에이전트가 놓인 상태를 함수로 표현한 형태  
  행동-가치 함수: 상태와 행동에 대한 가치를 함수로 표현한 형태  

[상태-가치 함수]  
MRP(Markov-Reward Process)의 가치 함수와 마찬가지로 상태 s에서 얻을 수 있는 리턴의 기댓값을 의미  
하지만, 차이점은 주어진 정책에 따라서 행동을 결정하고 다음 상태로 이동  
현재 시점의 상태-가치 함수가 다음 시점의 상태-가치 함수로 표현  

[행동-가치 함수]  
상태 s에서 a라는 행동을 취했을 때 얻을 수 있는 리턴의 기댓값  
가치 함수의 계산은 빅 오의 n의 3승으로 나타난다.  
그래서 상태 수가 많으면 계산이 어렵다.  

이 문제를 해결하는 방법은 4가지가 있다.  
1. 다이나믹 프로그램, 상태와 행동이 적고 모든 전이 확률을 알고 있다면
2. 몬테카를로, 최종 상태에서 가치를 추정하고, 중간 상태의 가치를 업데이트  
3. 시간 차 학습, 다이나믹 프로그래밍과 몬테카를로의 중간  
4. 함수적 접근 함수, 상태가 아주 많거나, 상태가 연속적인 값을 갖을 때 특성 벡터를 통해 해결

##### MDP를 위한 벨만 방정식
벨만 방정식은 상태-가치 함수와 행동-가치 함수의 관계를 나타내는 방정식  
- 벨만 기대 방정식  
- 벨만 최적 방정식

###### 벨만 기대 방정식
가치 함수: 어떤 상황에서 미래의 보상을 포함한 가치를 나타냄  
벨만 기대 방정식: 다음 상태로 이동하기 위해 어떤 정책에 따라 행동하는 것  
기댓값: 현재 기대되는 결과에 그 결과가 일어날 확률로 가중치가 곱해진 것과 같음  

R: 행동을 선택했을 때 얻어지는 보상  
할인율 * G: 미래에 일어날 모든 일의 평균  
P(s', r|s, a): 어떤 행동이 발생할 확률  

상태 s에서 행동 a를 했다.  
-> 상태 s에서 행동 a를 했을 때의 보상  
-> 그 다음 상태의 가치 함수, 할인율과 그 다음 상태로 전이될 확률 계산  

상태-가치 함수는 뒤따를 행동-가치 함수의 정책 기반 평균  
행동-가치 함수는 다음 상태-가치 함수에 대한 보상과 상태 전이 확률에 대한 결합 확률의 가중 평균  

[강화 학습 과정]  
1. 처음 에이전트가 접하는 상태 s나 행동 a는 임의의 값으로 설정  
2. 환경과 상호 작용하면서 얻은 보상과 상태에 대한 정보를 이용하여 어떤 상태에서 어떤 행동을 취하는 것이 좋은지 판단
3. 이때 최적의 행동을 판단하는 수단이 상태-가치 함수와 행동-가치 함수, 이것을 벨만 기대 방정식을 이용하여 업데이트하며 점점 좋은 보상을 얻을 수 있는 상태와 행동을 학습함
4. 2~3 과정 속에서 최대 보상을 갖는 행동을 선택하도록 최적화된 정책을 가짐

> 강화 학습은 가치 함수 초깃값(0 혹은 랜덤한 숫자)의 과정을 반복하며 얻은 정보를 업데이트하여 최적의 값을 얻는 것  
> 이렇게 반복저긍로 얻은 값이 가장 클 때 벨만 최적 방정식이라고 한다.  

###### 벨만 최적 방정식

###### 다이나믹 프로그래밍

##### 큐-러닝
###### 큐-러닝
###### 딥 큐-러닝

##### 몬테카를로 트리 탐색
###### 몬테카를로 트리 탐색 원리
###### 몬테카를로 트리 검색을 적용한 틱택토 게임 구현