# 20240611
## AI 드론봇 - 9

#### 자연어 전처리

##### 자연어 처리란
자연어 처리: 우리가 일상생활에서 사용하는 언어 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 과정  

- 딥러닝에 대한 이해 + 인간 언어에 대한 이해가 필요  
- 언어의 종류와 형태도 다양하여 난해 함  

스팸처리, 맞춤법 검사, 단어 검색, 객체 인식 등은 우수하나 질의 응답, 요약, 유사 단어 바꾸어 쓰기, 대화는 부족한 감이 있다.  
그런데 지금 2024년 6월 기준으로는 위 부족한 분야도 많이 보완한 것 같다.  


###### 자연어 처리 용어 및 과정
- 말뭉치(corpus): 자연어 처리에서 모델을 학습시키기 위한 데이터, 자연어 연구를 위해 특정한 목적에서 표본을 추출한 집합, 말뭉치는 메타데이터와 텍스트로 이루어진 인스턴스의 집합이다.
- 토큰: 자연어 처리를 위한 문자는 작은 단위로 나누어야 하는데 이때 문서를 나누는 단위
  + 토큰 생성: 문자열을 토큰으로 나누는 작업
  + 토큰 생성 함수: 문자열을 토큰으로 분리하는 함수
- 토큰화: 텍스트를 문장이나 단어로 분리하는 것
- 불용어(stop words): 문장 내에서 분석과는 관계가 없으나 많이 등장하는 단어
  ex. a, the, she, he 등
- 어간 추출(stemming): 단어를 기본 형태로 만드는 작업
  ex. consign, consigned, consigning -> consign으로 통일  
- 품사 태깅: 주어진 문장에서 품사를 식별하기 위해 붙여 주는 태그(식별 정보)
  + Det: 한정사
  + Noun: 명사
  + Verb: 동사
  + Perp: 전치사
  품사 태깅은 NLTK를 이용하는 데 `pip install nltk`

``` python
import nltk
nltk.download()
```

이 코드를 실행시키면 NLTK Download 팝업창이 뜨거나 작업 표시줄에 생기는데 이 녀석을 다운로드한 뒤, File > Exit 하면 진행할 수 있다.  

[nltk에서 사용하는 품사 약어]
- VBZ: 동사, 동명사, 또는 현재 분사
- PRP: 인칭 대명사(PP)
- JJ: 형용사
- VBG: 동사 또는 동명사 또는 현재 분사
- NNS: 명사, 복수형
- CC: 등위 접속사

[자연어 처리 과정]  
1. 인간 언어인 자연어가 입력 텍스트로 들어 옴
2. 입력한 테스트에 대한 전처리 과정
3. 전처리가 끝난 단어를 임베딩, 단어를 벡터로 변환하는 작업
4. 모델/모형을 이용하여 데이터에 대한 분류 및 예측을 수행


###### 자연어 처리를 위한 라이브러리
NLTK(Natural Language ToolKit): 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 라이브러리  

[주요 기능]  
- 말뭉치
- 토큰 생성
- 형태소 분석
- 품사 태깅

KoNLpy(코엔엘파이): 한국어 처리를 위한 파이썬 라이브러리, 기존에 공개된 꼬꼬마, 코모란, 한나눔, 트위터, 메카브 분석기를 한 번에 설치하고 동일한 방법으로 사용할 수 있게 함  
한국어로 작성한 공식 문서가 있으니 그것을 참고하면 쉽게 환경을 설치할 수 있을 것이다.  

[주요 기능]  
- 형태소 분석
- 품사 태깅

형태소: 단어를 쪼갤 대 의미를 가지는 최소 단위  
문장 > 어절 > 형태소 > 음절  

Gensim: 파이선에서 제공하는 워드투벡터 라이브러리  
[주요 기능]  
- 임베딩: 워드투벡터
- 토픽 모델링
- LDA(Latent Dirichlet Allocation): 주어진 문서에 대해 각 문서에 어떤 주제가 존재하는지 서술하는 확률적 토픽 모델 기법

`pip install -U gensim`으로 설치 가능하다.  

사이킷런: 파이선을 이용하여 문서를 전처리할 수 있는 라이브러리 제공, 특성 추출 용도로 많이 사용함  
[주요 기능]  
- CountVectorizer: 텍스트에서 단어의 등장 횟수를 기준으로 특성을 추출함  
- Tfidfvectorizer: TF-IDF 값을 사용해서 텍스트에서 특성을 추출
- HasingVectorizer: CouterVectorizer와 방법이 동일하지만 텍스트를 처리할 때 해시 함수를 사용하기 때문에 실행 시간이 감소함

##### 전처리
머신러닝이나 딥러닝에서 텍스트 자체를 특성으로 사용할 수 없기에 텍스트 데이터에 대한 전처리 작업이 필요  
전처리를 위해 토큰화, 불용어 제거, 어간 추출 등 작업 수행  

[전처리 과정]  
문장 -> 결측치 확인, 토큰화 -> 단어 색인 -> 불용어 제거 -> 축소된 단어 색인 -> 어간 추출  

###### 결측치 확인
결측치는 주어진 데이터 셋에서 데이터가 없는(Nan) 것  

###### 토큰화
주어진 텍스트를 단어/문자 단위로 자르는 것  
토큰화는 문장 토큰화와 단어 토큰화로 구분  
ex. 'A cat is on the sofa' -> 단어 토큰화 -> 'A', 'cat', 'is', 'on', 'the', 'sofa'  

문장 토큰화: 마침표, 느낌표, 물음표 등 문장의 마지막을 뜻하는 기호에 따라 분리되는 것  
단어 토큰화: 띄어쓰기를 기준으로 문장을 구분  

허나 한국어는 띄어쓰기만으로는 토큰을 구분하기 어려운 언어임  
어간과 어미 등의 요소 결합이 많기 때문으로 생각됨  

아포스트로피(')가 있는 문장은 `WordPunctTokenizer`를 이용함  

한글 토큰화는 KoNLpy 라이브러리를 사용하는 것이 바람직하다.  

gensim이랑 무언가 충돌해서 가상 환경을 새로 만들었다.  
`conda create -n 가상환경이름`으로 가상환경을 설치한다.  
그리고 `conda env list`로 가상 환경 목록을 확인할 수 있다.  

한글 형태소 분석을 위해 오픈 소스 한글 형태소 분석기(Twitter(Okt))를 사용함  

###### 불용어 제거
문장 내에서 빈번하게 발생하여 의미를 부여하기 어려운 단어, 불용어는 자연어 처리에서 효율성을 감소시키고, 처리 시간이 길어지는 단점이 존재  

###### 어간 추출
어간 추출(stemming)과 표제어 추출(lemmatiztion)은 단어의 원형을 찾아주는 것  

- 어간 추출은 단어 그 자체만 고려하므로 품사가 달라도 사용 가능  
- 표제어 추출은 단어가 문장 속에서 어떤 품사로 쓰였는지 고려하므로 품사가 같아야 사용이 가능함  

- 어간 추출은 사전에 없는 단어도 추출
- 표제어 추출은 사전에 있는 단어만 추출 가능

NLTK의 어간 추출 알고리즘: 포터, 랭커스터  
랭커스터 알고리즘은 포터 알고리즘과 달리 단어 원형을 알아보기 어려울 정도로 축소하므로 정확도가 낮음  
> 일반적인 상황보다 데이터셋을 축소시켜야 하는 상황에 주로 사용할 수 있음  

###### 정규화
정규화: 데이터셋이 가진 특성(혹은 칼럼)의 모든 데이터가 동일한 정도의 범위(스케일 혹은 중요도)를 갖도록 하는 것  
머신 러닝 / 딥러닝은 데이터 특성을 비교하여 패턴을 분석함  
그런데 한 컬럼의 값의 범위가 말도 안 되게 크다면, 그 컬럼이 분석에 큰 영향을 미칠 것이다.  
값이 크다고 분석에 더 중요한 요소가 아니므로 정규화를 함  

- MinMaxScaler() 모든 칼럼의 값이 0과 1 사이에 위치하도록 값의 범위를 조정, 이상치에 민감  
`(x - x에서 가장 작은 값) / (x에서 가장 큰 값 - x에서 가장 작은 값)`  
- StandardScaler(): 각 특성의 평균을 0, 분산을 1로 변경하여 컬럼 값의 범위를 조정
 `(입력 데이터 - 평균) / 표준 편차`  
- RobustScaler(): 평균과 분산 대신 중간 값과 ㅈ사분위수 범위를 사용, StandardScaler보다 정규화 이후 동일한 값이 퍼져있는 형태(첨도가 작은 형태)  
`사분위수 범위 = Q3 - Q1`  
- MaxAbsScaler(): 절댓값이 0~1 사이가 되도록 조정  
> 모든 데이터가 -1 ~ 1 사이가 되도록 조정하는 것이므로 양의 수로 구성된 데이터는 MinMaxScaler()와 유사하게 동작  

파이토치의 데이터셋과 데이터 로더를 활용하면 방대한 양의 데이터를 배치 단위로 쪼개 처리할 수 있으며, 데이터를 무작위로 섞을 수 있어 효율적으로 데이터를 처리할 수 있음  
> 데이터 양이 많을 때 주로 사용, 많지 않다면 반드시 사용하지 않아도 됨  

`nn.Linear()`에서 bias가 True인 것은 `y = wx`에서 +b를 추가하겠다는 것  

`nn.BCEWithLogitsLoss()`: 이진 분류에서 사용하는 손실함수는 이진 교차 엔트포리와 BCEWithLogitLoss 함수가 있다.  
`torch.nn.BCEWithLogitLoss = torch.nn.BCELoss + torch.sigmoid`  

`torch.round()`: 반올림을 할 때 사용  