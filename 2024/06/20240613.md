# 20240613
## AI 드론봇 - 11


#### 클러스터링
##### 클러스터링이란
어떤 데이터가 주어졌을 때, 특성이 비슷한 데이터끼리 묶어 주는 머신 러닝 기법  
머신 러닝 알고리즘에 딥러닝을 적용 -> 성능 향상 가능  

##### 클러스터링 알고리즘 유형
K-평균 군집화 알고리즘에 딥러닝을 결합한 방법과 다루지 않은 클러스터링 방법이 있다.  

###### K-평균 군집화
1. k개의 임의의 점 선택
2. 각 중심에 대한 거리 계산 -> 가까운 클러스터에 할당
3. 할당된 데이터 평균을 계산하여 새로운 클러스터 중심 결정
4. 클러스터 할당이 변경되지 않을 때까지 2~3을 반복

클러스터의 개수를 결정하는 것은 쉽지 않다.  
[클러스터의 개수를 편리하게 결정하는 방법]  
클러스터 개수와 WCSS(Within Cluster Sum of Squares)간 관계를 그래프로 표현한 후, WCSS 변경이 평평하게 하락하는 구간을 선택한다.  

WCSS: 모든 클러스터에 있는 각 데이터가 중심까지의 거리를 제곱하여 합을 계산  
엘보 그래프: K 값 범위에 대해 K-평균 알고리즘을 무작위로 초기화하고, 각 K 값을 WCSS에 플로팅  

[최적의 엘보를 찾는 방법]  
1. 곡선의 처음과 마지막 점을 직선으로 연결
2. 각 점에서 직선까지의 수직 거리를 계산
3. 가장 긴 거리를 엘보로 선정

파이토치는 머신 러니 알고리즘이 수행해야 할 일을 개발자가 일일이 함수로 구현해야 함  

`StandardScaler()`: 각 특성의 평균을 0, 분산을 1로 변경하여 특성의 스케일을 변경  

`torch.from_numpy()`: 넘파이 배열을 입력받아 텐서 형태로 변경  

``` python
import numpy as np
x = np.array([1, 2, 3])
y = torch.from_numpy(x)
print('텐서 결과: ', y)

# y 변수의 값을 변경했을 때 x에 어떤 영향을 미치는지 확인
y[0] = -1
print('넘파이 배열로 변경: ', y)
```

`kmeans()`: k-평균 군집화 알고리즘에 대한 훈련
- 첫 번째 파라미터: 훈련 데이터셋
- num_clusters: 클러스터 개수
- distance: 클러스터를 구성하기 위해 데이터 간 거리를 계산하는 방법
ex. 유클리드 거리, 맨해튼 거리, 민코프스키 거리, 코사인 유사도 등  
- device: GPU/CPU를 사용하도록 설정

`running k-means on cuda:0..
[running kmeans]: 4it [00:00, 13.83it/s, center_shift=0.000000, iteration=4, tol=0.000100]`

- center_shift: 유클리드 거리 계산을 이용하여 반복적으로 클러스터 중심을 변경, 더 이상 옮길 필요가 없을 때까지 반복
- iteration: 중지할 기준(tol)을 만족하면 학습을 멈추고 반복 횟수를 출력
- tol: tolerance의 약자로 학습을 중지할 기준이 되는 값



###### 가우시안 혼합 모델
가우시안 분포가 여러 개 혼합된 클러스터링 알고리즘  
현실에 있는 복잡한 형태의 확률 분포를 가우시안 분포 K개를 혼합하여 표현  

가우시안 혼합 모델을 이용한 분류는 주어진 데이터 x에 대해 그 데이터가 가우시안 분포에 속하는지 찾는 것  
만약 K번째 가우시안 분포에 속하면 1, 그렇지 않으면 0(책임도)    
가장 높은 값의 가우시안 분포를 선택  

베이즈 정리를 통해 각 데이터 점이 각 가우시안 분포에 속할 확률을 계산  
베이즈 정리: 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리  
.npy: 배열을 넘파이 형식으로 저장해놓은 것  

`np.save()`: 배열 한 개를 넘파이 형식의 바이너리 파일로 저장
`np.load()`: np.save()로 저장된 .npy를 배열로 불러 옴


``` python
# np.linspace, np.meshgird 사용법
# 생성되는 숫자의 개수는 num=50이 default
print(np.linspace(-1, 6, num=4))
print('-' * 50)

X, Y = np.meshgrid(np.linspace(-1, 6, 4), np.linspace(-1, 6, 4))

print(X)
print('-' * 50)
print(Y)

"""
[-1.          1.33333333  3.66666667  6.        ]
--------------------------------------------------
[[-1.          1.33333333  3.66666667  6.        ]
 [-1.          1.33333333  3.66666667  6.        ]
 [-1.          1.33333333  3.66666667  6.        ]
 [-1.          1.33333333  3.66666667  6.        ]]
--------------------------------------------------
[[-1.         -1.         -1.         -1.        ]
 [ 1.33333333  1.33333333  1.33333333  1.33333333]
 [ 3.66666667  3.66666667  3.66666667  3.66666667]
 [ 6.          6.          6.          6.        ]]
"""

``` python
# np.ravel()과 numpy.T의 활용
X, Y = np.meshgrid(np.linspace(-1, 6, 2), np.linspace(3, 4, 2))
print(X)
print('-' * 20)
print(Y)
print('-' * 20)
XX = np.array([X.ravel(), Y.ravel()])
print(XX)
print('-' * 20)
print(XX.T)
"""
[[-1.  6.]
 [-1.  6.]]
--------------------
[[3. 3.]
 [4. 4.]]
--------------------
[[-1.  6. -1.  6.]
 [ 3.  3.  4.  4.]]
--------------------
[[-1.  3.]
 [ 6.  3.]
 [-1.  4.]
 [ 6.  4.]]
"""
```

###### 자기 조직화 지도
SOM이라는 약치응로 쓰이며, 신경 생리학적 시스템을 모델링, 입력 패턴에 대해 정확한 정답을 주지 않고, 스스로 학습하여 클러스터링하는 알고리즘  

[SOM 구조]  
네트워크: 입력층, 2차원 격자로 된 경쟁층  
입력층과 경쟁층은 서로 연결, 가중치는 연결 강도(0~1 사이 정규화된 값)  

[학습 과정]  
1. 초기화: 모든 연결 가중치는 작은 임의의 값으로 초기화
2. 경쟁: 자기 조직화 지도는 경쟁 학습을 이용하여 입력층과 경쟁층을 연결. 연결 강도 벡터가 입력 벡터와 얼마나 가까운 지 계산하여 가장 가까운 뉴런이 승리하는 '승자 독점 방식' 사용  

가중치 테이블에서 i행, j열의 값과 입력벡터의 값이 가까울수록 연결 강도 벡터와 입력 벡터가 가까운 노드이다.  
연결 강도 벡터와 입력 벡터가 가장 가까운 뉴런으로 계산되면 그 뉴런의 이웃 뉴런도 학습

3. 협력: 승자 뉴런은 네트워크에서 가장 좋은 공간 위치를 차지, 승자와 함께 학습할 이웃 크기를 정의
4. 적응: 승리한 뉴런의 가중치와 이웃 뉴런을 업데이트  
5. 원하는 횟수만큼 2~3번 과정을 반복

`MiniSom()`: 시각화 기능이 거의 없는 SOM을 구현할 수 있는 라이브러리  
- 첫 번째 파라미터: SOM에서 x축에 대한 차원
- 두 번째 파라미터: SOM에서 y축에 대한 차원
- 세 번째 파라미터: 입력 벡터 개수
- sigma: 이웃 노드와의 인접 반경  
`sigma(t) = sigma / (1 + t / T), T는 반복 횟수 / 2`  
- learning_rate: 한 번 학습할 때 얼마큼 변화를 주는지에 대한 상수
`Learing rate(t) - learning rate / ((1+t) / (0.5*t)), t는 반복 구분자`