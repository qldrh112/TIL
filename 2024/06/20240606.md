# 20240606
## AI 드론봇 - 4
#### 4. 딥러닝 시작

##### 인공 신경망의 한계와 딥러닝 출현
오늘 날의 인공 신경망의 구조(입력층, 출력층, 가중치로 구성)는 퍼셉트론이라는 선형 분류기에 기반을 두고 있다.  

2차원 평면에 나타내보면 AND, OR는 선형으로, XOR는 비선형으로 나타난다.  
아마 이래서 XOR가 보안에 쓰이는 것 같다.  

그래서 XOR의 비선형을 극복하기 위해 학습이 가능하도록 다층 퍼셉트론을 고안했다.  
이 때 입력층과 출력층 사이에 다수의 은닉층이 여러 개 있는 신경망을 심층 신경망(DNN)이라고 한다.  
이는 딥러닝이라고도 한다. 

##### 딥러닝 구조  
여러 층을 가진 인공 신경망을 사용하여 학습을 수행하는 것을 딥러닝이라고 한다.  

###### 딥러닝 용어
딥러닝에서 계층은 3가지가 존재한다.
- 입력층: 데이터를 받아들이는 층
- 은닉층: 모든 입력 노드부터 값을 입력받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층
- 출력층: 신경망의 최종 결괏값이 포함된 층

가중치: 입력 값이 연산에 미치는 영향력을 조절하는 요소  
가중합, 전달 함수: 각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달되는데 이 때의 합, 가중합을 활성화 함수로 보내기에 전달 함수라고도 함  
활성화 함수: 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수, 시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등이 있다.  

- 시그모이드 함수: 선형 함수의 결가를 0~1 사이의 비선형 형태로 변형해줌, 주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용 -> 모델의 깊이가 깊어지면 기울기가 사라지는 기울기 소멸 문제로 인해 딥러닝 모델에서는 잘 사용하지 않는 추세
- 하이퍼볼릭 탄젠트 함수: 선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형해 줌, 시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만, 기울기 소멸 문제는 해결하지 못 하였음
- 렐루 함수: 입력(x)가 음수일 때는 0을 출력, 양수일 때는 x를 출력함, 경사 하강법에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않음, 하이퍼볼릭 탄젠트 함수보다 약 6배 정도 빠름, 다만, 음수인 경우 항상 0을 출력해서 학습 능력이 감소하는데 리키 렐루 함수로 해결하고자 함
- 리키 렐루 함수: 입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환합니다. 렐루 함수처럼 입력값이 수렴하는 구간이 제거됨  
- 소프트맥스 함수: 입력 값을 0~1 사이에 출력되도록 정규화화여 출력 값의 총합이 항상 1이 되도록 하는 것, 딥러닝에서 출력 노드의 활성화 함수로 많이 사용됨  

```python
# 렐루 함수와 소프트맥수 함수

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        # 은닉층, y = xA^T + b를 계산
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.relu = torch.nn.ReLu(inplace=True)
        # 출력층
        self.out = torch.nn.Linear(n_hidden, n_output)
        # 지정된 차원(dim)
        self.softmax = torch.nn.Softmax(dim=n_output)
    
    def forward(self, x):
        x = self.hidden(x)
        # 은닉층을 위한 렐루 활성화 함수
        x = self.relu(x)
        x = self.out(x)
        # 출력층을 위한 소프트맥스 활성화 함수
        x = self.softmax(x)
        return x
```

경사 하강법: 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트하는 방법  
> 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동하는 방법  
손실 함수: 경사 하강법에서 미분의 기울기를 이용하여 오차를 구하는 방법  
> 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표  

손실 함수의 값이 크면 추정과 실제의 괴리가 크다고 볼 수 있다.  

[손실 함수의 종류]
- 평균 제곱 오차: 실제 값과 예측 값의 차이를 제곱하여 평균을 낸 것

``` python
# 평균 제곱 오차

import torch

# reduction은 손실 값을 어떻게 계산할지 지정하는 매개 변수
# 'sum': 모든 요소의 손실 값을 합산한 값을 반환
loss_fn = torch.nn.MSELoss(reduction='sum')
y_pred = model(x)
loss = loss_fn(y_pred, y)
```

- 크로스 엔트로피 오차: 분류  문제에서 원-핫 인코딩했을 때만 사용할 수 있는 오차 계산법, 분류 문제에서 0과 1로 구분하기 위해 시그모이드를 사용하는데 이 때, 매끄럽지 못한 그래프가 발생하니 이것을 사용한다.  

``` python
# 크로스 엔트로피 오차

loss =  nn.CrossEntropyLoss()
# 평균이 0이고, 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성
# 5 x 6 크기의 배열을 생성, tensor의 그래디언트를 추적하고 업데이트 함
input = torch.randn(5, 6, requires_grad=True)
#  0~4까지의 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환
# 1 x 3 크기의 배열
target = torch.empty(3, dtype=torchl.long).random_(5)
output = loss(input, target)
output.backward()
```

텐서의 메서드 뒤에 `_`를 붙이는 이유: `inplace=True`와 같은 의미이다.  


###### 딥러닝 학습
학습은 크게 순전파와 역전파라는 2개의 단계로 진행한다.  
- 순전파: 네트워크에 훈련 데이터가 들어올 때 발생하며, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 지나감
- 역전파: 손실 함수의 비용이 계산되면 0에 가깝도록 하기 위해 역으로 전파되어 순전파의 가중치 값을 조정하는 데 사용됨


###### 딥러닝의 문제점과 해결 방안
딥러닝의 핵심: 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것  
> 활성화 함수가 적용된 은닉층 개수가 많을수록 데이터 분류가 잘 되고 있음을 알 수 있다.  

하지만 은닉층이 많은 경우 아래의 문제가 발생할 수 있다.  

1. 과적합: 훈련 데이터를 과하게 학습해서 발생, 훈련 데이터에 너무 맞춰져 있어, 실제 데이터의 분류에서 오차가 발생하는 현상
이를 해결하기 위해 학습 과정 중 일부 노드를 학습에서 제외함  

``` python
# drop out
class DropoutModel(torch.nn.Module):
    def __init__(self):
        super(DropoutModel, self).__init__()
        self.layer1 = torch.nn.Linear(784, 1200)
        # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미
        self.dropout1 = torch.nn.Dropout(0.5)
        self.layer2 = torch.nn.Linear(1200, 1200)
        self.dropout2 = torch.nn.Dropout(0.5)
        self.layer3 = torch.nn.Linear()
    
    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.dropout1(x)
        x = F.relu(self.layer2(x))
        x = self.dropout2(x)
        return self.layer3(x)
```

2. 기울기 소멸 문제: 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상  
이를 해결하기 위해 시그모이드나 하이퍼볼릭 탄젠트 대신 렐루 활성화 함수를 사용하여 해결할 수 있다.  

3. 성능이 나빠지는 문제 발생: 경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동하는 과정을 반복하는데 이 때 성능이 나빠지는 문제가 발생한다.  
이를 해결하기 위해 확률적 경사 하강법과 미니 배치 경사 하강법을 사용한다.  

[경사 하강법의 유형]  
- 배치 경사 하강법: 전체 데이터셋에 대한 오류를 구한 후, 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트하는 방법  
> 전체 훈련 데이터셋에 대해 가중치를 편미분 하는 방법  

- 확률적 경사 하강법: 임의로 선택한 데이터에 대해 기울기를 계산하는 방법으로 적은 데이터를 사용하므로 빠른 계산이 가능함

- 미니 배치 경사 하강법: 전체 데이터셋을 미니 배치 여러 개로 나누고, 미니 배치 하나마다 기울기를 구한 후, 그것의 평균 기울기를 이용하여 모델을 업데이트해서 사용하는 방법  

확률적 경사 하강법의 파라미터 변경 폭이 불안정한 문제를 해결하기 위해 학습 속도와 운동량을 조정하는 옵티마이저가 있다.
``` python
# 미니 배치 경사 하강법

class CustomDataset(Dataset):
    def __init__(self):
        self.x_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
        self.y_data = [[12], [18], [11]]

        def __len__(self):
            return len(self.x_data)
          
        def __getitem(self, idx):
            x = torch.FloatTensor(self.x_data[idx])
            y = torch.FloatTensor(self.y_data[idx])
            return x, y

dataset = CustomDataset()
dataloader DataLoader(
    # 데이터셋
    dataset,
    # 미니 배치 크기로 2의 제곱수를 사용하겠다는 의미
    batch_size=2,
    # 데이터를 불러올 때마다 랜덤으로 섞어서 가져옵니다.
)

```

###### 딥러닝을 사용할 때 이점
- 특성 추출: 데이터별로 어떤 특징을 가지고 있는지 찾아내고, 그것을 토대로 데이터를 벡터로 변환하는 작업
- 빅데이터의 효율적 활용: 딥러닝에서는 특성 추출을 알고리즘에 통합시킴, 빅데이터 사용을 통해 특성 추출의 성능 향상을 꾀했기 때문


##### 딥러닝 알고리즘
딥러닝 알고리즘은 심층 신경망를 사용함  

딥러닝에서 자주 사용하는 알고리즘은 CNN(합성곱 신경망)과 RNN(순환 신경망)
###### 심층 신경망
DNN이라고 하며 입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공 신경망  

다수의 은닉층을 통해 비선형적 관계를 학습할 수 있지만, 연산량 과다 문제와 기울기 소멸 문제가 존재함 -> 렐루 함수, 배치 정규화, 드롭 아웃 등 적용 필요  

###### 합성곱 신경망
CNN이라고 하며 합성곱층과 풀링층을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘, 영상 및 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치를 찾아내는 데 유용한 신경망  

[기존 신경망과의 차이점]  
- 각 층의 입출력 형상을 유지
- 이미지의 공간 정보를 유지하면서 인접 이미지와 차이가 있는 특징을 효과적으로 인식
- 복수 필터로 이미지의 특징을 추출하고 학습

###### 순환 신경망
시계열 데이터 같은 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망  
순환은 자기 자신을 참고한다는 것으로 현재 결과가 이전 결과를 참조함  

[순환 신경망의 특징]  
- 시간성을 가진 데이터가 많음
- 시간성 정보를 이용하여 데이터의 특징을 잘 다룸
- 시간에 따라 내용이 변하므로 데이터는 동적이고, 길이가 가변적
- 매우 긴 데이터를 처리하는 연구가 활발히 진행 중

순환 신경망은 기울기 소멸 문제가 있음 -> 이를 해결하고자 메모리 개념을 도입한 LSTM이 순환 신경망에서 사용  

순환 신경망은 자연어 처리 분야에 사용, 언어 모델링, 텍스트 생성, 자동 번역, 음성 인식, 이미지 캡션 생성  

###### 제한된 볼츠만 머신
가시층과 은닉층으로 구성된 모델  
가시층은 은닉층과만 연결 = 제한된 불츠만 머신  

[제한된 블츠만 머신의 특징]  
- 차원 감소, 분류, 선형 회귀 분석, 협업 필터링, 특성 값 학습, 주제 모델링에 사용
- 기울기 소멸 문제를 해결하기 위해 사전 학습 용도로 사용 가능
- 심층 신뢰 신경망(DBN)의 요소로 활용  

###### 심층 신뢰 신경망
입력층과 은닉층으로 구성된 제한된 블츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망  
> 사전 훈련된 제한된 블츠만 머신을 층층이 쌓아올린 구조, 레이블이 없는 데이터에 대한 비지도 학습 가능  

[학습 절차]  
1. 가시층과 은닉층 1에 제한된 블츠만 머신을 사전 훈련
2. 첫 번째 층 입력 데이터와 파라미터를 고정하여 두 번째 층 제한된 블츠만 머신을 사전 훈련
3. 원하는 층 개수만큼 제한된 블츠만 머신을 쌓아올려 전체 DBN을 완성

[심층 신뢰 신경망의 특징]  
- 순차적으로 심층 신뢰 신경망을 학습시켜 가며 계층적 구조 생성
- 비지도 학습 방식
- 위로 올라갈수록 추상적 특성 추출
- 학습된 가중치를 다층 퍼셉트론의 가중치 초깃값으로 사용

##### 우리는 무엇을 배워야 할까?
간단한 선형 회귀 분류를 이용하여 원하는 값 도출 가능: 머신 러닝  
복잡한 비선형 데이터에 대한 분류 및 예측: 딥러닝  
