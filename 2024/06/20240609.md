# 20240609
## AI 드론봇 - 7
#### 시계열 분석

##### 시계열 문제
시계열 분석이란 시간에 따라 변하는 데이터를 사용하여 추이를 분석하는 것  
> 추세를 파악하거나 향후 전망을 예측하기 위한 용도로 사용  

[데이터 변동 유형에 따른 구분]  
- 불규칙 변동: 어떤 규칙성이 없어 예측 불가능하고, 우연적으로 발생하는 변동
ex. 전쟁, 홍수, 화재, 지진, 파업 등  

- 추세 변동: 시계열 자료가 갖는 장기적인 변화 추세
ex. 국내총생산, 인구증가율  
추세: 장기간에 걸쳐 지속적으로 증감하거나 일정한 상태를 유지하려는 성향  

- 순환 변동: 2~3년 정도의 일정한 기간을 주기로 순환적으로 나타나는 변동
> 1년 이내 주기로 곡선을 그리며 추세 변동에 따라 움직이는 것  
ex. 경기 변동  

- 계절 변동: 계절적 영향과 사회적 관습에 따라 1년 주기로 발생하는 것

규칙적 시계열과 불규치적 시계열로 나눌 수 있다.  
시계열 데이터를 잘 분석한다는 것은 불규칙성을 갖는 시계열 데이터에 특정한 기법이나 모델을 적용하여 규칙적인 패턴을 찾거나 예측하는 것을 의미  

여러 모델이 있지만, 최근에는 딥러닝을 이용하여 시계열 데이터의 연속성을 기계 스스로 찾아내도록 하는 방법이 더 좋은 성능을 내고 있다고 한다.  

##### AR, MA, ARMA, ARIMA
시계열 분석은 독립 변수를 사용하여 종속 변수를 예측하는 일반적인 머신 러닝에서 시간을 독립 변수로 사용한다는 특징이 있다.  
이런 특성으로 인해 일반적인 방법과는 차이가 존재한다.  

###### AR 모델
AutoRegressive(자기 회귀)모델은 이전 관측 값이 이후 관측 값에 영향을 준다는 아이디어 대한 모형  

`시계열 데이터에서 현재 시점 = (과거가 현재에 미치는 영향을 나타내는 모수 * 시계열 데이터의 과거 시점) + 시계열 분석에서 오차 항(백색 잡음)`  
> p 시점을 기준으로 그 이전의 데이터에 의해 현재 시점의 데이터가 영향을 받는 모형  

###### MA 모델
Moving Average(이동 평균)모델은 트렌드가 변화하는 상황에 적합한 회귀 모델  
시계열을 따라 윈도우 크기만큼 슬라이딩하는 것이 특징  

`시계열 데이터에서 현재 시점 = (매개 변수 * 과거 시점의 오차) + 오차항`  
이전 데이터의 상태가 아닌 이전 데이터의 오차로 현재 데이터의 상태를 추론하겠다.  

###### ARMA 모델
AutoRegressvie Moving Average(자기 회귀 이동 평균) 모델은 AR과 MA를 섞은 모델, 연구 기관에서 주로 사용  
AR, MA 두 가지 관점에서 과거의 데이터를 사용하는 것(오차와 상태를 모두 반영하겠사옵니다.)  

###### ARIMA 모델
자기 회귀와 이동 평균을 둘 다 고려하는 모형, ARMA와 달리 선형 관계뿐만 아니라 추세까지 고려한 모델  

[구현 절차]  
1. ARIMA() 함수를 호출하여 사용  
  - ARIMA(p, d, q): p: 자기 회귀 차수, d: 차분 차수, q: 이동 평균 차수
2. fit() 메서드를 호출하고, 모델에 데이터를 적용하여 훈련
3. predict() 메서드를 호출하여 미래의 추세 및 동향에 대해 예측

statsmodels 라이브러리: 검정 및 추정, 회귀 분석, 시계열 분석 기능을 제공하는 파이썬 패키지  

자기 회귀 차수가 5, 차분 차수가 1이라는 것은 과거 5개의 시점의 값에 영향을 받으며, 시계열 데이터를 한 번 차분하여 정상성 시계열로 반환한다는 의미  

[`read_csv()`의 파라미터]   
  * header:  csv 파일의 n번째 행을 열 이름으로 사용할 것인지
  * parse_dates: csv 파일의 n번째 열을 날짜로 파싱
  * index_col: csv 파일의 n번째 열을 인덱스로 설정, 0이면 인덱스를 따로 두지 않겠다.
  * squeeze: True일 때 데이터가 하나의 열로 구성된 경우 Series 형태로 반환 -> 판다스 2.0부터 삭제되어 df.squeeze()를 사용
  * data_parser: 날짜를 파싱하는 사용자 정의 함수를 지정

plot(kind='kde')에서 kde는 커널 밀도 추정의 약자, 데이터 분포를 부드러운 선 그래프로 나타냄  
np.nan_to_num(x): 배열 x의 NaN 값을 0으로 무한대 값을 매우 큰 숫자로 변환하는 메서드  

[SARIMAX.summary()의 요소]  
- 종속 변수 (Dep. Variable)
- 관측치 수 (No. Observations): 36 (36개의 데이터 포인트)
- 로그 우도 (Log Likelihood), 모델이 주어진 값을 설명할 수 있는 정도
- AIC (Akaike Information Criterion), 모델의 복잡성과 적합도를 평가하는 기준, 값이 작을 수록 그 모델이 더 적합함
- BIC (Bayesian Information Criterion), AIC와 유사, 모델의 파라미터 수에 더 큰 페널티 부여(과적합 방지)
- HQIC (Hannan-Quinn Information Criterion), AIC, BIC와 유사, 모델 적합성과 적합도 사이의 균형을 찾기 위한 기준

회귀 계수: 모델이 각 시차 항이 현재 값에 미치는 영향을 나타냄  
- ar.L1: -0.8788 (첫 번째 시차의 자기회귀 계수), 양수면 정의 관계, 음수면 음의 관계
  * 표준 오차 (std err)
  * z-값
  * P>|z|
  * 이후의 값 2개 = 신뢰 구간  
- sigma2 (오차의 분산)

Ljung-Box (L1) (Q): 0.39 (잔차의 자기상관을 테스트)  
Prob(Q): 0.53 (잔차가 독립적이지 않음을 나타낼 통계적 유의성이 없음) 

Jarque-Bera (JB): 1.60 (잔차의 정규성을 테스트)
Prob(JB): 0.45 (잔차가 정규 분포에서 벗어나지 않음을 나타낼 통계적 유의성이 없음)  

Heteroskedasticity (H): 1.43 (잔차의 이분산성을 테스트)  
Prob(H) (two-sided): 0.54 (잔차가 이분산성을 보일 통계적 유의성이 없음)

Skew: 0.38 (잔차의 비대칭성)  
Kurtosis: 2.28 (잔차의 첨도)  

``` python
# 필요한 라이브러리 임포트
from pandas import read_csv
from pandas import to_datetime
from pandas import DataFrame
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# 데이터를 불러올 때 날짜 형식을 명시적으로 지정
df = read_csv('./data/sales.csv', header=0, index_col=0)
df.index = to_datetime(df.index, format='%m-%d')
series = df.squeeze('columns')

# 인덱스 주기 설정(시계열 데이터의 인덱스를 일별 주기로 설정)
series.index = series.index.to_period('D')

# ARIMA 모델 적용
model = ARIMA(series, order=(5, 1, 0))
model_fit = model.fit()
print(model_fit.summary())

# DF에 모델에 대한 오차 정보를 residuals이라는 변수에 저장
residuals = DataFrame(model_fit.resid)
residuals.plot()
plt.show()
residuals.plot(kind='kde')
plt.show()
print(residuals.describe())
```

``` python
# statsmodels 라이브러리를 이요한 sales 데어테셋 예측
import numpy as np
from pandas import read_csv
from pandas import to_datetime
from matplotlib import pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

df = read_csv('./data/sales.csv', index_col=0, header=0)
df.index = to_datetime(df.index, format='%m-%d')
series = df.squeeze('columns')

X = series.values
X = np.nan_to_num(X)
size = int(len(X) * 0.66)
train, test = X[:size], X[size:]
history = [x for x in train]
predictions = list()

# 훈련 데이터 학습 -> 예측 -> 예측 값을 다시 훈련 데이터에
for t in range(len(test)):
    model = ARIMA(history, order=(5, 1, 0))
    model_fit = model.fit()
    # forecast() 메서드를 사용하여 예측 수행
    output = model_fit.forecast()
    # 모델 출력 결과를 yhat에 저장
    yhat = output[0]
    predictions.append(yhat)
    obs = test[t]
    history.append(obs)
    print(f'predicted={yhat}, expected={obs}')

# 손실 함수로 평균 제곱 오차 사용
error = mean_squared_error(test, predictions)
print(f'Test MSE {error:.3f}')
plt.plot(test)
plt.plot(predictions, color='red')
plt.show()
```

하지만, 요즘은 딥러닝 기반의 시계열 모델이 태동되며 사장되었다.  

##### 순환 신경망(RNN)
시간적으로 연속성이 있는 데이터를 처리하려고 고안된 인공 신경망  

RNN의 Recurrent(반복되는)은 이전 은닉층이 현재 은닉층의 입력이 되면서 반복되는 순환 구조를 가짐을 의미  
RNN이 기존의 네트워크와 다른 점은 기억을 갖는다는 것  

[RNN의 입출력에 따른 분류]  
- 일대일: 순환이 없기 때문에 RNN이라고 말하기 어려움, 순방향 네트워크  
- 일대다: 입력이 하나이고, 출력이 다수인 구조, 이미지를 입력해서 이미지에 대한 설명을 문장으로 출력하는 이미지 캡션
- 다대일: 입력이 다수이고, 출력이 하나인 구조, 문장을 입력해서 긍정/부정을 출력하는 감성 분석기에서 사용  
- 다대다: 입력과 출력이 다수인 구조, 언어를 번역하는 자동 번역기 등이 대표적인 사례


###### RNN 계층과 셀
RNN 계층은 입력된 배치 순서대로 모두 처리  
RNN 셀은 오직 하나의 단게만 처리함  

RNN 계층은 셀을 래핑하여 동일한 셀을 여러 단계에 적용  

[셀 유형]  
- nn.RNNCell: simpleRNN 계층에 대응되는 RNN 셀
- nn.GRUCell: GRU 계층에 대응되는 GRU 셀
- nn.LSTMCell: LSTM 계층에 대응되는 LSTM 셀

RNN은 대표적으로 자연어 처리에 활용된다.  

##### RNN 구조
RNN은 은닉층 노드가 연결되어 이전 단계 정보를 은닉층 노드에 저장할 수 있도록 구성한 신경망  

W__(xh)는 입력층에서 은닉층으로 전달되는 가중치  
W__(hh)는 t 시점의 은닉층에서 t+1시점의 은닉층으로 전달되는 가중치  
W__(hy)는 은닉층에서 출력층으로 전달되는 가중치  

RNN에서는 모든 가중치는 동일하다.  

[t단계에서 RNN 계산]  
1. 은닉층 계산을 위해 현재 시점의 입력값과 바로 전 시점의 은닉층이 필요하다.
`(이전 은닉층 x 은닉층 -> 은닉층 가중치 + 입력층 -> 은닉층 가중치 x (현재) 입력 값), 하이퍼볼릭 탄젠트 활성화 함수`  

2. 출력층은 심층 신경망과 계산 방법이 동일하다.  
은닉층 -> (출력층 가중치 x 현재 은닉층)에 소프트맥스 함수를 적용

3. RNN의 오차는 심층 신경망에서 전방향 학습과 달리 각 단계마다 오차를 추정, 각 단계마다 실제 값과 예측 값으로 오차(MSE)을 이용하여 측정

4. 역전파는 BPTT를 이용하여 모든 단계마다 처음부터 끝까지 역전파를 이용하여 모든 단계마다 처음부터 끝까지 역전파를 보냄
BPTT: 각 단계(t)마다 오차를 측정하고 이전 단계로 전파  
계산한 오차를 이용하여 W와 바이어스를 업데이트함  
이때, 오차가 멀리 전파될 때, 계산량이 많아지고, 전파되는 양이 점차 적어지는 문제점이 발생  
> 이를 보완하기 위해 오차를 몇 단계까지만 전파하는 생략된 BPTT를 사용하거나 LSTM, GRU를 사용

###### RNN 셀 구현
`torchtext`는 자연어 처리 분야에서 사용하는 데이터로더, 파일 가져오기, 토큰화, 단어 집합 생성, 인코딩, 단어 벡터 생성 등의 작업 지원

`torchtext.lagacy.data.Field()`: 데이터 전처리를 위해 사용되는 메서드
- lower: 대문자를 모두 소문자로 변경함
- fix_length: n의 길이의 데이터를 얻을 수 있음
- batch_first: 신경망에 입력되는 텐서의 첫 번째 차원 값이 배치 크기가 되도록 함

모델의 네트워크로 입력되는 데이터는 [시퀀스 길이, 배치 크기, 은닉층의 뉴런 개수]  
batch_first가 true이면 [배치 크기, 시퀀스 길이, 은닉층의 뉴런 개수]로 변경  
은닉층의 입력 데이터: [은닉층 개수, 배치 크기, 은닉층의 뉴런 개수]  

<b>파이토치는 각 계층별 데이어틔 형태를 맞추는 것에서 시작하여 끝날 정도로 중요하다</b>

- sequential: 데이터에 순서가 있는지 나타냄

`datasets.IMDB.splits(TEXT, LABEL)`  
pytorch의 datasets에는 사용자의 학습을 위한 다양한 데이터셋을 제공하고 있음(datasets.IMDB 등)
- splits: 전체 데이터셋을 텍스트와 레이블로 분할, 이후 텍스트 데이터셋은 훈련 용도로, 레이블은 테스트 용도로 사용

단어 집합: IMDB 데이터셋에 포함된 단어를 이용하여 하나의 딕셔너리와 같은 집합을 만드는 것, 단어 집합은 중복을 제거한다.  

`buill_vocab()`: 단어 집합을 생성하는 메서드  
- 첫 번째 파라미터: 훈련 데이터셋
- max_size: 단어 집합의 크기로 단어 집합에 포함되는 어휘 수
- min_freq: 훈련 데이터셋에서 특정 단어의 최소 등장 횟수
ex. min_freq가 10일 때, 훈련 데이터셋에서 최소 10번 이상 등장한 것만 단어 집합에 추가하겠다.  
- vectors: 임베딩 벡터를 지정할 수 있음  
사전 학습된 임베딩으로 워드투벡터, 글로브 등이 있고, 파이토치에서도 `nn.embedding()`을 통해 단어를 랜덤한 숫자 값으로 변환 후 가중치를 학습하는 방법 제공  

일반적으로 계층의 유닛 개수를 늘리는 것보다 계층 자체에 대한 개수를 늘리는 것이 성능을 위해 더 좋다.  

은닉층 층수가 인공 신경망이 비선형 문제를 좀 더 잘 학습하도록 하는 반면, 층 안의 포함된 뉴런은 가중치와 바이어스를 계산하는 용도로 사용  

하지만, 최적의 은닉층 개수와 유닛 개수를 찾는 것은 어렵다.  
그래서 필요보다 많은 층과 유닛을 구성하고 과적합이 발생하지 않도록 개수를 조정하는 방식을 많이 사용함  

`BucketIterator`는 데이터로더와 쓰임새가 같음  
배치 크기 단위로 값을 차례대로 꺼내어 가져오고 싶을 때 사용  
- 첫 번째 파라미터: 배치 크기 단위로 데이터를 가져올 데이터셋
- batch_size: 한 번에 가져올 데이터 크기(배치 크기)
- device: GPU를 사용할 것인지 CPU를 사용할 것인지 지정

`nn.RNNCell()`: RNN 셀 구현을 위한 구문  
- input_dim: 훈련 데이터셋의 특성 개수(배치, 입력 데이터 칼럼 개수 / 특성 개수(batch, input_size)) 형태를 갖음
- hidden_size: 은닉층의 뉴런(유닛) 개수로 (배치, 은닉층의 뉴런 개수(batch, hidden_size)) 형태를 갖음

`ht = self.rnn(word, ht)`: 재귀적으로 발생하는 상태 값을 처리하기 위한 구문
- ht: 현재 상태를 의미하는 것
- word: 현재의 입력 벡터
 
`nn.Embedding()`: 임베딩 처리를 위한 구문
- 첫 번째 파라미터: 임베딩을 할 단어의 수(단어 집합의 크기)
- embeding_dim: 임베딩할 벡터의 차원

`nn.LogSoftmax`: 소프트맥스 함수의 결과에 로그를 취함
`nn.NLLLoss`: 다중 분류에 사용
`torch.nn.CrossEntropyLoss`는 다중 분류에 사용, 위 두 개의 조합으로 구성  

모델 학습을 위한 함수는  
1. 데이터로더에서 데이터를 가져와서 모델에 적용
2. 손실 함수를 적용하여 오차를 구함
3. 옵티마이저를 이용하여 파라미터(가중치, 바이어스)를 업데이트

에포크마다 모델이 훈련 데이터에 대해 전체적으로 한 번 학습함  

학습과 검증 데이터셋에 대한 오차가 유사하면 과적합은 발생하지 않는다.  
과적합이 발생하면 어떻게 되는가?  
훈련 데이터셋에 대한 오차는 매우 낮아지지만, 그것에 딱 드러맞기 때문에 검증 데이터의 오차는 오히려 증가한다.  

###### RNN 계층 구현
`nn.RNN() RNN 계층에 대한 구문
- embed_dim: 훈련 데이터셋의 특성 개수(칼럼 개수)
- self.hidden_dim: 은닉 계층의 뉴런(유닛) 개수
- num_layers: RNN 계층의 개수
- batch_first: True로 설정된다면 (시퀀스의 길이, 배치 크기, 특성 개수) -> (배치 크기, 시퀀스의 길이, 특성 개수)  

`init_state(batch_size=x.size(0))`: RNN의 초기 은닉 상태를 생성하는 함수, 이 코드를 통해 RNN의 시작 지점을 알 수 있다.  

forward 함수는 모델의 순전파를 정의  
rnn(입력, 이전 은닉 상태 값)  

`sigmoid()`: 신경망에서 사용하는 활성화 함수, 입력 값을 0과 1사이의 값으로 변환  
- 입력 값의 크기에 따라 출력 값이 비선형적으로 변화  
- 기울기 소실 문제가 발생할 수 있다.  
ex. 이진 분류 문제에서 각 클래스에 속할 확률을 계산할 때 사용  

`corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()`: 모델의 정확도를 구하는 코드
- max(1)[1]: `max(dim=0)[0]`은 최대값을 나타내고, `max(dim=0)[1]`은 최댓값을 갖는 데이터의 인덱스를 나타냄
- view(y.size()): `logit.max(1)[1]`의 결과를 y.size()로 크기를 변경함
- data == y.data:  모델의 예측 결과 `logit.max(1)[1].view(y.size()).data`가 레이블 값과 같은지 비교

하이퍼 파라미터: 학습률이나 배치 크기처럼 사용자가 임의로 지정해야 하는 값  

##### LSTM
RNN의 결정적 단점: 가중치가 업데이트 되는 과정에서 기울기가 1보다 작은 값이 계속 곱해지기에 기울기가 사라지는 기울기 소멸 문제  

이를 해결하기 위해 LSTM이나 GRU 같은 확장된 RNN 방식을 활용하고 있음  

###### LSTM 구조
LSTM 순전파: 기울기 소멸 문제를 해결하기 위해 망각 게이트, 입력 게이트, 출력 게이트라는 새로운 요소를 은닉층의 각 뉴런에 추가  

- 망각 게이트: 과거 정보를 어느 정도 기억할 지 결정, 과거 정보와 현재 데이터를 입력받아 시그모이드를 취한 후 그 값을 과거 정보에 곱해 줌 -> 0이면 과거 정보는 버리고, 1이면 과거 정보를 온전히 보존  

- 입력 게이트: 현재 정보를 기억하기 위함, 과거 정보와 현재 데이터를 입력 받아 시그모이드와 하이퍼볼릭 탄젠트 함수를 기반으로 현재 정보에 대한 보존량을 결정  

하이퍼볼릭 탄젠트 함수: 입력 값을 범위(-1, 1)로 압축하여 활성화 함수로 사용, 평균이 0이며 미분값이 최대가 되는 형태를 가지는 S자 형태의 함수, 시그모이드 함수에 비해 더 넓은 출력 범위를 가짐  

- 셀: 각 단계에 대한 은닉 노드를 메모리 셀, 총합을 사용하여 셀 값을 반영하며, 이것으로 기울기 소멸 문제를 해결할 수 있음  
망각 게이트와 입력 게이트의 이전 단계 셀 정보를 계산하여 현재 단계의 셀 상태를 업데이트함  

- 출력 게이트: 과거 정보와 현재 데이터를 사용하여 뉴런의 출력을 결정함  이전 은닉 상태와 t번째 입력을 고려해서 다음 은닉 상태를 계산함  

LSTM에서는 이 은닉 상태가 그 시점의 출력이 됨  

1이면 의미있는 결과로 최종 출력, 0이면 연산 출력을 하지 않음  

LSTM 역전파: 셀을 통해 역전파를 수행하기에 중단 없는 기울기라고 한다.  
셀 내부에서도 입력 방향으로 전파된다.  

###### LSTM 셀 구현
`torch.chunk`는 텐서를 쪼갤 때 사용하는 함수  
- 첫 번째 파라미터: 텐서를 몇 개로 쪼갤지 설정
- 두 번째 파라미터: 어떤 차원을 기준으로 쪼갤지 결정, dim=1이므로 열 단위로 텐서를 분할하겠다는 의미

`uniform()`: 난수를 위해 사용(=`randint(), random()`)

`nn.Linear(입력특징수, 출력특징수, bias=True)`: 선형 변환 계층을 정의, 입력 데이터와 가중치 매트릭스의 곱셈을 통해 연관성을 나타내는 선형 변환을 수행  

``` python
from random import *

# 1과 10 사이의 임의의 정수
ri = randint(1, 10)
print(ri)

# 0부터 1 사이의 임의의 실수
rd = random()
print(rd)

# 1부터 10 사이의 임의의 실수(float)
ui = uniform(1, 10)
print(ui)

# 1부터 10 사이를 2씩 건너뛴 임의의 정수
rr = randrange(1, 10, 2)
print(rr)

"""
1
0.9904462077669746
2.6686762833515956
1
"""
```

`torch.squeeze()`: 텐서의 차원을 줄이고자 사용
``` python
import torch

# (2 x 1)크기의 2차원 텐서 생성
x = torch.FloatTensor([[1], [2]])
print(x)
print(x.shape)
print('------squeeze 적용------')
# squeeze()가 적용되어 1차원으로 축소
print(x.squeeze())
print(x.squeeze().shape)

"""
tensor([[1.],
        [2.]])
torch.Size([2, 1])
------squeeze 적용------
tensor([1., 2.])
torch.Size([2])
"""
```

셀 상태는 입력, 망각, 셀 게이트에 의해 결정, 은닉층은 출력 게이트에 의해 결정
`torch.mul()`: 텐서에 곱셈을 할 때 사용
``` python
import torch

x = torch.FloatTensor([[1], [2]])
print(x)
print('------mul 적용------')
# x라는 텐서의 원소에 3을 곱함
torch.mul(x, 3)
"""
tensor([[1.],
        [2.]])
------mul 적용------
tensor([[3.],
        [6.]])
"""
```

DataLoader를 통해 데이터셋을 RAM으로 가져오는데 너무 데이터셋이 크면 batch 단위로 데이터를 로드한다.  
`len(train_dataset) / batch_size`: 한 에포크 동안 배치의 개수  

`Xavier 초기화`: 1을 은닉층의 크기의 제곱근으로 나누는 방법, 분산을 제어하여 학습을 안정적으로 가능하게 함  

`self.parameters()`: `nn.Module` 클래스에서 제공하는 메서드, 모델의 모든 학습 가능한 매개 변수를 반환  

cy `F.tanh`로 스케일링하여 최종 은닉 상태를 계산할 대, tanh 함수가 적용됨   

Variable(): 텐서를 감사서 해당 텐서에 적용된 연산을 기록하는 데 사용, 그래디언트를 필요로 하는 텐서를 생성하기 위해 사용  

`torch.manual_seed(125)`: 모든 장치에서 무작위 수를 생성하는 시드를 설정  
`torch.cuda.manual_seed_all(125)`: 모든 cuda 장치에서 무작위 수를 생성하는 시드를 설정  

c0: LSTM의 초기 셀 상태, 시퀀스 처리할 때마다 초기화, 0으로 초기화되며 hidden state와 동일한 크기를 가짐
cn: LSTM의 순방향 전달 과정에서 각 시간 단계마다의 셀 상태, 시퀀스를 처리할 때마다 업데이트

전처리되지 않은 데이터셋은 정확도를 높이기 위해 은닉층의 개수 및 하이퍼파라미터 수정 등이 필요하다.  

모델의 정확도가 박살났다면, 들여쓰기가 제대로 되었는지 확인해보거라  

###### LSTM 계층 구현
모델이 얼마나 주가 예측을 잘 하는지 알아보자  
숫자 데이터는 임베딩이 필요하지만, 단어로 이루어진 데이터셋은 임베딩 과정을 거쳐야 함  

날짜 컬럼은 임베딩 처리하기 어려우므로 인덱스로 바꿔먹는 것이 용이함  

`MinMaxScaler`: 주로 데이터가 정규 분포를 따르지 않을 때 사용
`StandardScaler`: 주로 데이터가 정규 분포를 따를 때 사용

`torch.zeros`: 주어진 크기로 모든 원소가 0인 텐서를 생성, 각 계층마다 배치 크기와 은닉 상태 크기에 해당하는 0으로 초기화된 텐서를 만듦  
`x.size(0)`: 입력 배치의 크기

> 각 계층마다 배치 크기와 은닉 상태 크기에 해당하는 0으로 초기화된 텐서를 만드는 것  

`forward()`에서 x?: 모델의 입력 데이터  
x의 형태는 (batch_size, seq_length, input_size) 

ex. 배치 크기(한 번에 처리할 데이터의 수)가 32, 시퀀스 길이(타임스텝의 수)가 10, 입력 크기(각 타임 스텝에서의 입력 벡터의 크기)가 50이라면 x의 형태는 (32, 10, 50)  

`self.lstm(x, (h_0, c_0))`은 시퀀스 데이터 x를 한 타임스텝씩 처리하며 각 타임스텝마다 은닉 상태와 셀 상태를 업데이트 함  

`hn = hn.view(-1, self.hidden_size)`  
hn: LSTM의 마지막 계층에서 최종 은닉 상태, (num_layers, batch_size, hidden_size)로 구성됨  
완전 연결층에 입력하기 위해서는 (batch_size, hidden_size) 형태로 변환해야 함  
그래서 view(-1, self.hidden_size)가 맞다.  

`axvline()`: 그래프의 축을 따라 수직성을 표현할 때 사용
`axhline()`: 그래프의 축을 따라 수평선을 표현할 때 사용


##### 게이트 순환 신경망(GRU)
Gated REcurrent Unit는 게이트 메커니즘이 적용된 RNN 프레임워크의 한 종류이면서 LSTM보다 구조가 간단  

###### GRU 구조
LSTM에서 사용하는 망각 게이트와 입력 게이트를 하나로 합친 것  
별도의 업데이트 게이트로 구성되어 있다.  

하나의 게이트 컨트롤러가 망각 게이트와 입력 게이트를 모두 제어함  
게이트 컨트롤러가 1을 출력하면 망각 게이트는 열리고, 입력 게이트는 닫힘  
0을 출력하면 망각 게이트는 닫히고, 입력 게이트는 열림  
> 이전 기억이 저장될 때마다 단계별 입력은 삭제  

GRU는 출력 게이트가 없어 전체 상태 벡터가 매 단계마다 출력, 이전 어느 상태의 어느 부분이 출력될지 제어하는 새로운 게이트 컨트롤러가 별도로 존재  

- 망각 게이트: 과거 정보를 적당히 초기화하려는 목적으로 시그모이드 함수를 출력으로 이용하여 (0, 1) 값을 이전 은닉층에 곱함
이전 시점의 은닉층 값에 현시점의 정보에 대한 가중치를 곱한 것  

- 업데이트 게이트: 과거와 현재 정보의 최신화 비율을 결정하는 역할. 시그모이드로 출력된 결과는 현시점의 정보량을 결정하고, 1에서 뺀 값을 직접 시점의 은닉층 정보와 곱함

- 후보군: 현시점의 정보에 대한 후보군을 계산, 과거 은닉층의 정보를 그대로 이용하지 않고 망각 게이트의 결과를 이용하여 후보군을 계산

- 은닉층 계산: 업데이트 게이트 결과와 후보군 결과를 결합하여 현시점의 은닉층을 계산, 시그모이드를 통해 현시점에서 결과에 대한 정보량, 1-시그모이드를 통해 과거의 정보량을 결정  


###### GRU 셀 구현
``` python
# 라이브러리 호출
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dataset
from torch.autograd import Variable
from torch.nn import Parameter
from torch import Tensor
import torch.nn.functional as F
from torch.utils.data import DataLoader
import math

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
cuda = True if torch.cuda.is_available() else False
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

torch.manual_seed(125)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(125)

# 데이터 전처리
mnist_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (1.0,))
])

# 데이터셋 내려받기 및 전처리 적용
from torchvision.datasets import MNIST

download_root = '../data/MINST_DATASET'

train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)
valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)
test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)

# 데이터셋 메모리로 가져오기
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size, shuffle=True)
# 일반적으로 검증과 테스트 용도의 데이터셋은 섞어서 사용하지 않지만, 다양한 학습을 위해 True로 지정
valid_loader = DataLoader(valid_dataset, batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size, shuffle=True)

# 변수 값 선정
batch_size, n_iter = 100, 6000
num_epochs = n_iter / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

# GRU 셀 네트워크
class GRUcell(nn.Module):

    def __init__(self, input_size, hidden_size, bias=True):
        super(GRUcell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias
        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)
        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)
        self.reset_parameters()

    def reset_parameters(self):
        std = 1.0 / math.sqrt(self.hidden_size)
        for w in self.parameters():
            w.data.uniform_(-std, std)
        
    def forward(self, x, hidden):
        x = x.view(-1, x.size(1))

        # LSTM 셀에서는 gates를 x2h + h2h로 정의했지만, GRU에서는 개별적인 상태를 유지
        gate_x = self.x2h(x)
        gate_h = self.h2h(hidden)
        gate_x = gate_x.squeeze()
        gate_h = gate_h.squeeze()

        # 총 세 개의 게이트(망각, 입력, 새로운 게이트)를 위해 세 개로 쪼갭니다.
        i_r, i_i, i_n = gate_x.chunk(3, 1)
        h_r, h_i, h_n = gate_x.chunk(3, 1)

        resetgate = F.sigmoid(i_r + h_r)
        inputgate = F.sigmoid(i_i + h_i)
        # 새로운 게이트는 탄젠트 활성화 함수가 적용된 게이트
        newgate = F.tanh(i_n + (resetgate * h_n))

        hy = newgate + inputgate * (hidden - newgate)
        return hy
    
# 전반적인 네트워크 구조
class GRUModel(nn.Module):

    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):
        super(GRUModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim
        # 앞에서 정의한 GRUcell 함수를 불러옵니다.
        self.gru_cell = GRUcell(input_dim, hidden_dim, layer_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        if torch.cuda.is_available():
            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())
        else:
            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))
    
        outs = []
        # LSTM 셀에서는 셀 상태에 대해서도 정의했었지만, GPU 셀에서는 셀은 사용하지 않음
        hn = h0[0, :, :]

        for seq in range(x.size(1)):
            hn = self.gru_cell(x[:, seq, :], hn)
            outs.append(hn)
        out = outs[-1].squeeze()
        out = self.fc(out)
        return out
    
# 옵티마이저와 손실 함수 설정
input_dim, hidden_dim, layer_dim, output_dim = 28, 128, 1, 10

model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)

if torch.cuda.is_available():
    model.cuda()

criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters() , lr=learning_rate)

# 모델 학습 및 성능 검증
seq_dim, iter = 28, 0
loss_list = []

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        if torch.cuda.is_available():
            images = Variable(images.view(-1, seq_dim, input_dim).cuda())
            labels = Variable(labels.cuda())
        else:
            images = Variable(images.view(-1, seq_dim, input_dim))
            labels = Variable(labels)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        if torch.cuda.is_available():
            loss.cuda()
        
        loss.backward()
        optimizer.step()

        loss_list.append(loss.item())
        iter += 1

        if iter % 500 == 0:
            correct, total = 0, 0
            for images, labels in valid_loader:
                if torch.cuda.is_available():
                    images = Variable(images.view(-1, seq_dim, input_dim).cuda())
                else:
                    images = Variable(images.view(-1, seq_dim, input_dim))
                
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)

                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum()
                else:
                    correct += (predicted == labels).sum()
                
            accuracy = 100 * correct / total
            print(f'Iteration: {iter}. Loss: {loss.item()}. Accuracy: {accuracy}')

# 테스트 데이터셋을 이용한 모델 예측
def evaluate(model, val_iter):
    corrects, total, total_loss = 0, 0, 0
    model.eval()
    for images, labels in val_iter:
        if torch.cuda.is_available():
            images = Variable(images.view(-1, seq_dim, input_dim).cuda())
            labels = Variable(labels.cuda())
        else:
            images = Variable(images.view(-1, seq_dim, input_dim))
            lables = Variable(labels)

        logit = model(images).to(device)
        loss = F.cross_entropy(logit, labels, reduction='sum')
        _, predicted = torch.max(logit.data, 1)
        total += labels.size(0)
        total_loss += loss.item()
        if torch.cuda.is_available():
            corrects += (predicted.cpu() == labels.cpu()).sum()
        else:
            corrects += (predicted == labels).sum()

    avg_loss = total_loss / len(val_iter.dataset)
    avg_accuracy = corrects / total
    return avg_loss, avg_accuracy

test_loss, test_acc = evaluate(model, test_loader)
print(f'Test Loss: {test_loss:5.2f} | Test Accuracy: {test_acc:5.2f}')
```

###### GRU 계층 구현
판다스의 컬럼은 대소문자를 구분하니 참고할 것  

`output, (hn) = self.gru(x, (h_0))`: GRU 레이어를 호출하여 입력 x를 처리함  

gru 레이어는 output(모든 시퀀스 타임스텝의 출력), hn(마지막 타임스텝의 은닉 상태)를 반환  
변수에 괄호를 치는 것은 하나의 값이 아니라 여러 값일 때 구분하기 위해 사용하는 듯 하다.

##### 양방향 RNN
RNN은 이전 시점의 데이터를 참고해서 정답을 예측하지만, 실제 문제에서는 과거 시점이 아닌 미래 시점의 데이터에 힌트가 있는 경우도 있다.  
양방향 RNN은 이전 시점 데이터뿐만 아니라 이후 시점의 데이터도 함깨 활용하여 출력 값을 예측하고자 하는 것  

###### 양방향 RNN 구조
하나의 출력 값을 예측하는 데 메모리 셀 두 개를 사용
- 첫 번째 메모리 셀: 이전 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산
- 두 번째 메모리 셀: 다음 시점의 은닉 상태를 전달받아, 현재의 은닉 상태를 계산
> 두 개를 모두 출력층에서 출력 값을 예측하는 데 사용

###### 양방향 RNN 구현
`data.set_index('Date', inplace=True)`: set_index 할 때 컬럼명만 적어라 그렇지 않으면 Date 칼럼이 남아 있음  

`nn.Module`에는 신경망 모듈을 초기화하는 데 필요한 다양한 설정과 속성이 정의되어 있다.  

양방향 LSTM은 `nn.LSTM()`에서 bidirectional을 true로 설정하면 끝  
이는 양방향 RNN과 GRU에서 모두 적용된다.  

레이어(계층의 개수)는 하나지만, 입력 데이터가 전방향과 역방향 학습에 모두 전달되며 모두 출력에 반영되기 때문에 한 번 학습에 2개의 계층이 필요하므로 2를 곱해준다.  

安峰 先生 曰: 과적합은 훈련을 시킴에도 불구하고, 정확도가 떨어지는 것  

시계열 데이터는 일반적으로 숫자 나열보다는 한글 및 영문으로 자연어로 구현된 데이터가 대부분이기 때문이다.  
시계열 구현에서 가장 중요한 것은 데이터에 대한 전처리.  