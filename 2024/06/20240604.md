# 20240604
## AI 드론봇 - 2
#### 2. 실습 환경 설정과 파이토치 기초

##### 파이토치 개요
2017년 공개된 딥러닝 프레임워크로 루아 언어로 개발된 토치를 페이스북에서 파이썬 버전으로 내놓은 것  

###### 파이토치 특징 및 장점
GPU에서 텐서 조작 및 동적 신경망 구축이 가능한 프레임워크  
- GPU: 연산 속도를 빠르게 하는 역할  
- 텐서: 파이토치의 데이터 형태로 단일 데이터 형식으로 된 자료의 다차원 행렬  
- 동적 신경망: 훈련을 반복할 때마다 네트워크 변경이 가능한 신경망  

[파이토치의 장점]  
- 효율적인 계산
- 성능(낮은 CPU 활용)  
- 직관적인 인터페이스

###### 파이토치의 아키텍처
[파이토치의 계층]  
- 파이토치 API
  - torch: GPU를 지원하는 텐서 패키지
  - torch.autograd: 자동 미분 패키지
  - torch.nn: 신경망 구축 및 훈련 패키지
  - torch.multiprocessing: 파이썬 멀티 프로세싱 패키지
  - torch.utils: DataLoader 및 기타 유틸리티를 제공하는 패키지
- 파이토치 엔진
  - Autograd C++
  - Aten C++
  - JIT C++
  - Python API  
- 텐서, 연산 처리  
  - CPU, GPU(TH(토치), THC(토치 CUDA))를 이요하여 효율적인 데이터 구조, 다차원 텐서에 대한 연산을 처리함  

##### 파이토치 기초 문법
텐서를 정확히 다뤄야 문제를 해결할 수 있고, 신경망에서 데이터 입/출력을 제어할 수 있다.  

###### 텐서 다루기

``` python
# 텐서 생성
import torch
print(torch.tensor([[1, 2], [3, 4]])) # 2차원 형태의 텐서
print(torch.tensor([[1, 2], [3, 4]], device='cuda:0')) # GPU에 텐서 생성
print(torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)) # dtype를 이용하여 텐서 생성


# 텐서를 ndarray로 변환
temp = torch.tensor([[1, 2], [3, 4]])
print(temp.numpy())

temp = torch.tensor([[1, 2], [3, 4]], device='cuda:0')
print(temp.to('cpu').numpy())
```

``` python
# 텐서의 인덱스 조작
temp = torch.FloatTensor([1, 2, 3, 4, 5, 6, 7]) # 파이토치로 1차원 벡터 생성
print(temp[0], temp[1], temp[-1]) # tensor(1.) tensor(2.) tensor(7.)
print(temp[2:5], temp[4:-1]) # tensor([3., 4., 5.]) tensor([5., 6.])
```

``` python
# 텐서 연산 및 차원 조작
v = torch.tensor([1, 2, 3])
w = torch.tensor([3, 4, 6])
print(w - v) # 길이가 같은 벡터 간 뺄셈 연산
"""
tensor([2, 2, 3])
"""

temp = torch.tensor([[1, 2], [3, 4]])
print(temp.shape) # torch.Size([2, 2])
print(temp.view(4, -1)) # 2x2 행렬을 4x1 행렬로 변환
print(temp.view(-1)) # 2x2 행렬을 1차원 벡터로 변환
print(temp.view(1, -1)) # 2x2 행렬을 (1, ?) 행렬로 변환 -> 4 = 1 * ? -> 1x4 행렬로 변환
print(temp.view(-1, 1)) # 2x2 행렬을 (?, 1) 행렬로 변환 -> 4 = ? * 1 -> 4x1 행렬로 변환
```

###### 데이터 준비
데이터 호출 방법 = 파이썬 라이브러리를 활용(판다스), 파이토치에서 제공하는 데이터를 이용하는 방법  

1. 단순하게 파일을 불러와서 사용하기  
``` python
import pandas as pd
import torch
data = pd.read_csv('../class2.csv')

x = torch.from_numpy(data['x'].values).unsqueeze(dim=1).float() # csv 파일의 x칼럼의 값을 넘파이로 배열로 받아 Tensor(dtype)으로 변환
y = torch.from_numpy(data['y'].values).unsqueeze(dim=1).float()
```

2. 커스텀 데이터셋을 만들어서 사용  
``` python
class CustomDataset(torch.utils.data.Dataset):
    # csv_file 파라미터를 통해 데이터셋을 불러옵니다.
    def __init__(self, csv_file):
        self.label = pd.read_csv(csv_file)
    # 전체 데이터셋의 크기를 반환합니다.
    def __len__(self):
        return len(self.label)
    # 전체 x와 y 데이터 중에 해당 idx번째 데이터를 가져옵니다.
    def __getitem__(self, idx):
      sample = torch.tensor(self.label.iloc[idx, 0:3]).int()
      label = torch.tensor(self.lable.iloc[idx, 3]).int()
      return sample, label

tensor_dataset = CustomDataset('../covtype.csv')
# 데이터셋을 torch.utils.data.DataLoader에 파라미터로 전달합니다.
dataset = DataLoader(tensor_dataset, batch_size=4, suffle=True)
```

3. 파이토치에서 제공하는 데이터셋 사용
``` python
import torchvision.transforms as transforms

# 평균이 0.5 표준편차가 1이 되도록 데이터의 분포를 조정
mnist_transform = transforms.Compost([
  transforms.ToTensor(),
  transforms.Normalize((0.5, ), (1.0, ))
])

from torchvision.dataset import MNIST
import requests
# 내려받을 경로 지정
download_root = '../chap02/data/MNIST_DATASET'

# 훈련 데이터셋, 검증 데이터셋, 테스트 데이터셋
trans_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)
valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)
test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)
```

###### 모델 정의
계층: 모듈 또는 모듈을 구성하는 하나의 단위, 합성곱층, 선형 계층 등이 있다.  
모듈: 한 개 이상의 계층이 모여서 구성된 것으로 모듈이 모여 새로운 모듈을 만들 수 있다.  
모델: 최종적으로 원하는 네트워크로 한 개의 모듈이 모델이 될 수 있다.  

1. 단순 신경망을 정의하는 방법  
`model = nn.Linear(in_features=1, out_features=1, bias=True)`  

2. nn.Module()을 상속하여 정의하는 방법  
``` python
class MLP(Module):
    # 모델에서 사용할 모듈과 활성화 함수를 정의
    def __init__(self, inputs):
        super(MLP, self).__init__()
        # 계층 정의
        self.layer = Linear(inputs, 1)
        # 활성화 함수 정의
        self.activation = Sigmoid()

    # 모델에서 실행되어야 하는 연산 정의
    def forward(self, X):
        X = self.layer(X)
        X = self.activation(X)
        return x
```

3. Sequential 신경망을 정의하는 방법  

``` python
import torch.nn as nn
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)
            nn.ReLU(inplace=True)
            nn.MaxPool2d(2))

        self.layer2 = nn.Sequential(
          nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5),
          nn.ReLU(inplace=True)
          nn.MaxPool2d(2))
        
        self.layer3 = nn.Sequential(
          nn.Linear(in_features=30*5*5, out_features=10, bias=True),
          nn.ReLU(inplace=True))

        def forward(self, x):
            x = self.layer1(x)
            x = self.layer2(x)
            x = x.view(x.shape[0], -1)
            x = self.layer3(x)
            return x
# 모델에 대한 객체 생성
model = MLP()

print('Printing children\n------------------')
# 같은 수준의 하위 노드를 반환함
print(list(model.children()))
print('\n\nPrinting Modules\n----------------')
# 모델의 테으워크에 대한 모든 노드를 반환함
print(list(model.modules()))
```

4. 함수로 신경망을 정의하는 방법  
``` python
def MLP(in_features=1, hidden_features=20, out_features=1):
    hidden = nn.Linear(in_features=in_features, out_features=hidden_features, bias=True)
    activation = nn.ReLU()
    output = nn.Linear(in_features=hidden_features, out_features=out_features, bias=True)
    net = nn.Sequential(hidden, activation, ouptut)
    return net
```

###### 모델의 파라미터 정의
- 손실함수: 학습하는 동안 출력과 실제 값 사이의 오차를 측정함  
=> wx + b를 계산한 값과 실제 값인 y의 오차를 구해서 모델의 정확성을 측정합니다.  
- 옵티마이저: 데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.  
- 학습률 스케쥴러: 미리 지정된 횟수의 에포크를 지날 대마다 학습률을 감소시켜줌  
=> 초기에는 빠른 학습을 진행 -> 최소점 근처에 다다르면 최적점을 찾아갈 수 있게 학습률을 감소  

``` python
# 모델의 파라미터를 정의하는 예시코드
from torch.optim import optimizer
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), Ir=0.01, momentum=0.9)
scheduler = torch.optim.Ir_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)

# 에포크 수만큼 데이터를 반복하여 처리
for epoch in range(1, 100+1):
    # 배치 크기만큼 데이터를 가져와서 학습 진행
    for x, y in dataloader:
        optimzer.zero_grad()

loss_fn(model(x), y).backyard()
optimizer.step()
scheduler.step()
```

###### 모델 훈련
학습을 시킨다 = y = wx + b라는 함수에서 w와 b의 적절한 값을 찾는다는 의미  

``` python
# 모델을 훈련시키는 예시 코드
for epoch in range(100):
    yhat = model(x_train)
    loss = criterion(yhat, y_train)
    # 오차가 중점적으로 쌓이지 않도록 초기화
    optimizer.zero_grad() 
    loss.backward()
    optimizer.step()
```

###### 모델 평가
주어진 테스트 데이터셋을 사용하여 모델을 평가함  

1. 함수를 사용하여 모델을 평가하는 방법  
``` python
import torch
import torchmetrics

preds = torch.randn(10, 5).softmax(dim=-1)
target = torch.randint(5, (10, ))

# 모델을 평가하기 위해 torchmetrics.functional.accuracy 이용
acc = torchmetrics.functional.accuracy(preds, target)
```

2. 모듈을 이용하여 모델을 평가하는 방법
``` python
import torch
import torchmetrics
# 모델 정확도 초기화
matric = torchmetrics.Accuracy()

n_batches = 10
for i in range(n_batches):
    preds = torch.randn(10, 5).softmax(dim=1)
    target = torch.randint(5, (10,))

    # 현재 배치에서 모델 평가(정확도)
    acc = metric(predsd, target)
    print(f'Accuracy on batch {i}: {acc}')

# 모든 배치에서 모델 평가(정확도)
acc = metric.compute()
print(f'Accuracy on all data: {acc}')
```

###### 훈련 과정 모니터링
학습 진행 과정에서 각 파라미터에 어떤 값이 어떻게 변화하는지 모니터링하기 위해 텐서보드를 사용한다.  
사용 방법은 아래와 같다.  
1. 텐서보드를 설정(set up)  
2. 텐서보드를 기록(write)  
3. 텐서보드르 사용하여 모델 구조를 살펴봅니다.  

``` python
# 텐서보드 사용하기 
import torch
from torch.utils.tensorboard import SummaryWriter
# 모니터링에 필요한 값이 저장될 위치
writer = SummaryWriter('../chap02/tensorboard')

for epoch in range(num_epochs):
    # 학습 모드로 전환
    model.train()
    batch_loss = 0.0

    for i, (x, y) in enumerate(dataloader):
        x, y = x.to(device).float(), y.to(device).float()
        outputs = model(x)
        loss = creterion(outputs, y)
        # 스칼라 값(오차)를 기록
        writer.add_scalar('Loss', loss, epoch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
# SummeryWriter가 더 이상 필요하지 않으면 close() 메서드 호출  
writer.close()
```
이후 `tensorboard --logdir=../chap02/tensorboard --port=6006` 명령을 입력하고 `http://localhost:6006`를 입력하면 웹 페이지가 열린다.  


##### 실습 환경 설정
###### 아나콘다 설치
설치는 최신 버전으로 해라 어짜피 파이썬 버전은 가상 환경 만들어서 설정할 수 있다.  
환경 변수는 반드시 등록하기  

###### 가상 환경 생성 및 파이토치 설치
`conda create -n 가상환경이름 python=파이썬 버전`: 가상 환경 만들기  
`conda env list`: 가상 환경 확인
`activate torch_book`: 가상 환경 활성화  
`conda env remove -n 가상환경이름`: 해당 가상 환경 삭제  
`conda install ipykernel`: 생성된 가상 환경에 커널 설치
`python -m ipykernel install --user --name torch_book --display-name "torch_book"`: 가상환경에 커널 연결  

`conda install pytorch==원하는 버전 torchvision==원하는 버전 torchaudio==원하는버전 -c pytorch` 혹은  
`pip install torch==원하는버전 torchvision==원하는버전 torchchaudio==원하는버전`  

`pip install jupyter notebook`: 주피터 노트북 설치
`jyupiter notebook`: 주피터 노트북 실행  

##### 파이토치 코드 맛보기
``` python
# 필요한 라이브러리 호출

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# 데이터 호출
dataset = pd.read_csv('./data/car_evaluation.csv')
dataset.head()

# 데이터셋 분포 확인

# 그래프의 사이즈를 변수에 지정
fig_size = plt.rcParams['figure.figsize']
fig_size[0] = 8
fig_size[1] = 6
plt.rcParams['figure.figsize'] = fig_size
# output 열의 각 빈도를 파이 형태로 표현해라
# explode: 파이 차트 요소 간 거리 주기
dataset.output.value_counts().plot(kind='pie', autopct='%0.05f%%',
                                   colors=['lightblue', 'lightgreen', 'orange', 'pink'], explode=(0.05, 0.05, 0.05, 0.05))

# 데이터를 범주형 타입으로 변환

# 데이터셋 컬럼 리스트
categorical_columns = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety']

for category in categorical_columns:
    # astype() 메서드를 이용하여 데이터를 범주형으로 변환
    dataset[category] = dataset[category].astype('category')

# 범주형 데이터를 텐서로 변환하기 위해서는 다음과 같은 절차가 필요함  
# 범주형 데이터 -> dataset[category] -> 넘파이 배열 -> 텐서
# cat.codes: 범주형 데이터(단어)를 숫자(넘파이 배열)로 변환하기 위해 사용(어떤 숫자가 어떤 클래스로 매핑되어 있는지 확인이 어려움)
price = dataset['price'].cat.codes.values
maint = dataset['maint'].cat.codes.values
doors = dataset['doors'].cat.codes.values
persons = dataset['persons'].cat.codes.values
lug_capacity = dataset['lug_capacity'].cat.codes.values
safety = dataset['safety'].cat.codes.values

# np.stack: 2개 이상의 넘파이 객체를 합칠 때 사용함
categorical_data = np.stack([price, maint, doors, persons, lug_capacity, safety], 1)
# 합친 넘파이 배열 중 열 개의 행을 출력하여 보여줌
categorical_data[:10]

# 배열을 텐서로 변환

categorical_data = torch.tensor(categorical_data, dtype=torch.int64)
categorical_data[:10]

# 레이블로 사용할 칼럼을 텐서로 변환

# get_dummies: 가변수로 만들어주는 함수(문자를 (0, 1)로 바꾸어주겠다.)
# 열을 더 만들어서 해당하면 true, 해당하지 않으면 false로 바꾸는 거임
outputs = pd.get_dummies(dataset.output)
outputs = outputs.values
# 1차원 텐서로 변환
outputs = torch.tensor(outputs).flatten()

print(categorical_data.shape)
print(outputs.shape)

# 범주형 칼럼을 N차원으로 변환

categorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]
categorical_column_sizes
cateogrical_embedding_sizes = [(col_size, min(50, (col_size+1) // 2)) for col_size in categorical_column_sizes]

print(cateogrical_embedding_sizes)

# 데이터셋 분리

total_records = 1728
# 전체 데이터 중 20%를 테스트 용도로 사용
test_records = int(total_records * .2)

categorical_train_data = categorical_data[:total_records - test_records]
categorical_test_data = categorical_data[(total_records - test_records):total_records]
train_outputs = outputs[:total_records - test_records]
test_outputs = outputs[(total_records - test_records):total_records]

# 데이터셋 분리 확인

print(len(categorical_train_data))
print(len(train_outputs))
print(len(categorical_test_data))
print(len(test_outputs))

# 모델의 네트워크 생성

# 클래스 형태로 구현되는 모델은 nn.Module을 상속받음
class Model(nn.Module):
    def __init__(self, embedding_size, output_size, layers, p=0.4):
        """
        self: 자기 자신
        embedding_size: 범주형 컬럼의 임베딩 크기
        output_size: 출력층의 크기
        layers: 모든 계층에 대한 목록
        p: 드롭아웃(default=0.5)
        """
        # Model 클래스에 접근
        super().__init__()
        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])
        self.embedding_dropout = nn.Dropout(p)

        all_layers = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        # 입력층의 크기를 찾기 위해 범주형 컬럼 개수를 input_size 변수에 저장
        input_size = num_categorical_cols

        # 모델의 네트워크 계층 구축을 위해 각 계층을 all_layers 목록에 추가
        for i in layers:
            # 선형 계층: 입력 데이터에 선형 변환, y = Wx + b
            all_layers.append(nn.Linear(input_size, i))
            # 활성화 함수
            all_layers.append(nn.ReLU(inplace=True))
            # 배치 정규화(평균을 0, 분산을 1)
            all_layers.append(nn.BatchNorm1d(i))
            # 과적합 방지에 사용
            all_layers.append(nn.Dropout(p))
            input_size = i
        
        all_layers.append(nn.Linear(layers[-1], output_size))
        # 신경망의 계층에 대한 목록(all_layers)를 nn.Sequential 클래스로 전달
        self.layers = nn.Sequential(*all_layers)

# 학습데이터를 받아 연산을 진행하는 함수, 모델 객체를 데이터와 함께 호출하면 자동으로 실행
    def forward(self, x_categorical):
        embeddings = []
        for i, e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:, i]))

        # 넘파이의 concatenate와 같지만 대상이 텐서
        x = torch.cat(embeddings, 1)
        x = self.embedding_dropout(x)
        x = self.layers(x)

        # 모델의 출력을 원하는 장치로 이동
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        x = x.to(device)

        return x

# Model 클래스의 객체 생성

model = Model(cateogrical_embedding_sizes, 4, [200, 100, 50], p=0.4)
print(model)

# 모델의 파라미터 정의
# 데이터를 분류하기 위해 크로스 엔트로피 손실 함수, 옵티마이저는 아담을 사용

loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# CPU, GPU 사용 지정

if torch.cuda.is_available():
    # GPU가 있으면 GPU를 사용
    device = torch.device('cuda')
else:
    # GPU가 없으면 CPU를 사용
    device = torch.device('cpu')

# 모델 학습

epochs = 500
aggregated_losses = []
train_outputs = train_outputs.to(device=device, dtype=torch.int64)

# for 문은 500회 반복되며, 각 반복마다 손실 함수가 오차를 계산
for i in range(epochs):
    i += 1
    y_pred = model(categorical_train_data)
    single_loss = loss_function(y_pred, train_outputs)
    # 반복할 때마다 오차를 aggregated_losses에 추가
    aggregated_losses.append(single_loss)

    if i % 25 == 1:
        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')

    optimizer.zero_grad()
    # 가중치를 업데이트 하기 위해 손실 함수의 backward() 메서드 호출
    single_loss.backward()
    # 옵티마이저 함수의 step() 메서드를 이용하여 기울기 업데이트
    optimizer.step()

# 오차가 25 에포크마다 출력된다.
print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')


# 테이스 데이터셋으로 모델 예측

test_outputs = test_outputs.to(device=device, dtype=torch.int64)
with torch.no_grad():
    y_val = model(categorical_test_data)
    loss = loss_function(y_val, test_outputs)
print(f'Loss: {loss:.8f}')

# 모델의 예측 확인

print(y_val[:5])

# 가장 큰 값을 가지는 인덱스 확인
y_val = np.argmax(y_val.cpu().numpy(), axis=1)
print(y_val[:5])

# 테스트 데이터셋을 이용한 정확도 확인

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print(confusion_matrix(test_outputs.cpu(), y_val))
print(classification_report(test_outputs.cpu(), y_val))
# 정확도
print(accuracy_score(test_outputs.cpu(), y_val))
```


