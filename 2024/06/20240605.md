# 20240605
## AI 드론봇 - 3
#### 3. 머신 러닝 핵심 알고리즘

##### 지도 학습
- 분류: 주어진 데이터를 정해진 범주에 따라 분류하는 것(ex. 메일을 스팸 메일로 분류하는 것)  
- 회귀: 데이터의 특성을 기준으로 연속된 값을 그래프로 표현하여 패턴이나 트렌드를 예측하는 것(ex. 주가를 예측하는 것)  

###### K-최근접 이웃
새로운 입력을 받았을 때, 기존 클러스터에서 모든 데이터와 인스턴스 기반 거리를 측정한 후, 가장 많은 속성을 가진 클러스터에 할당하는 분류 알고리즘  

사용 이유: 주어진 데이터에 대한 분류  
언제 사용할까?: 직관적이고, 사용하기 쉬움, 훈련 데이터를 충분히 확보할 수 있는 환경에서 사용하기  

``` python
# 라이브러리 호출 및 데이터 준비

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import metrics

# 데이터셋의 열 이름 할당
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

dataset = pd.read_csv('./data/iris.data', names=names)

# 훈련 및 테스트 데이터셋 분리

# 모든 행 사용, 열은 마지막 하나만 빼고 사용
X = dataset.iloc[:, :-1].values
# 모든 행 사용, 열은 5번째 것만 가져오기
y = dataset.iloc[:, 4].values

from sklearn.model_selection import train_test_split
# X, y를 사용하여 훈련과 테스트 데이터셋으로 분리하며, 테스트 데이터셋의 비율은 20%만 사용
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.preprocessing import StandardScaler
# 특성 스케일링 평균이 0, 표준편차가 1이 되도록 변환
s = StandardScaler()

# 훈련 데이터를 스케일링 처리
X_train = s.fit_transform(X_train)

# 테스트 데이터를 스케일링 처리
X_test = s.transform(X_test)

# 모델 생성 및 훈련

from sklearn.neighbors import KNeighborsClassifier

# K=50인 K-최근접 이웃 모델 생성
knn = KNeighborsClassifier(n_neighbors=50)
# 모델 훈련
knn.fit(X_train, y_train)

# 모델 정확도

from sklearn.metrics import accuracy_score

y_pred = knn.predict(X_test)
print(f'정확도: {accuracy_score(y_test, y_pred)}')

# 최적의 K 찾기
k = 10
acc_array = np.zeros(k)

# 1~10
for k in np.arange(1, k+1, 1):
    classifier = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    acc = metrics.accuracy_score(y_test, y_pred)
    acc_array[k-1] = acc

max_acc = np.amax(acc_array)
acc_list = list(acc_array)
k = acc_list.index(max_acc)
print(f'정확도 {max_acc}으로 최적의 k는 {k+1}입니다.')
```

K의 값이 50일 때, 정확도가 n%이고, K의 값이 1일 때는 정확도는 m%이다.  
이와 같이 K-최근접 이웃 알고리즘은 K 값에 따라서 성능이 달라지므로 초기 설정이 중요함

###### 서포트 벡터 머신
분류를 위한 기준선을 정의하는 모델  

사용 이유: 주어진 데이터에 대한 분류
언제 사용할까?: 커널만 적절히 선택하면 정확도가 대단히 좋기에 정확도를 요구하는 분류 문제를 해결할 때 용이함, 텍스트 분류에도 사용함  

- 하드 마진: 이상치를 허용하지 않는 것  
- 소프트 마진: 어느 정도의 이상치를 포함하는 것을 허용하는 것  


TP_CPP_MIN_LOG_LEVEL이라는 환경 변수를 통해 로깅을 제어(기본 값은 0으로 모든 로그가 표시됨)  
INFO 로그를 필투링하면 1, WARNING 로그를 필터링 하면 2, ERROR 로그를 추가로 필터링하면 3으로 설정  

환경 변수를 바꾸어 가면서 실행하는 것도 학습에 좋음  

``` python
# 라이브러리 호출

from sklearn import svm
from sklearn import metrics
from sklearn import datasets
from sklearn import model_selection
import os

os.environ['TP_CPP_MIN_LOG_LEVEL'] = '3'

# iris 데이터를 준비하고 훈련과 테스트 데이터셋으로 분리

iris = datasets.load_iris()
# 수행 시 결과값을 동일하게 만들어주는 random_state 인수(값으로 들어가는 숫자는 중요하지 않다.)
X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.6, random_state=42)

# SVM 모델에 대한 정확도


svm  = svm.SVC(kernel='linear', C=1.0, gamma=0.5)
# 훈련 데이터를 사용하여 SVM 분류기를 훈련
svm.fit(X_train, y_train)
# 훈련된 모델을 사용하여 테스트 데이터에서 예측
predictions = svm.predict(X_test)
score = metrics.accuracy_score(y_test, predictions)
# 테스트 데이터(예측 정확도 측정), 소수점 모든 자리수를 출력
print(f'정확도: {score:0f}')
```

SVM은 선형 분류와 비선형 분류를 지원함  
비선형에 대한 커널은 선형으로 분류할 수 없는 데이터로 인해 발생함  

버선형 데이터를 해결하는 기본적인 방법은 저차원 데이터 -> 고차원 데이터로 보내는 것  
> 많은 수학적인 계산 필요(성능 문제 발생 가능성)  

- 선형 커널: 선형으로 분류 가능한 데이터에 적용  
`K(a, b) = a**r * b (a, b는 입력 벡터)`  

- 다항식 커널: 실제로 특성을 추가하지 않지만, 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있는 방법  
`K(a, b) = (ya**T * b)**d (a, b는 입력벡터, y는 감마, d는 차원, 이때 y,d 는 하이퍼 파라미터)`  

- 가우시안 RBF 커널: 다항식 커널의 확장, 입력 벡터를 차원이 무한한 고차원으로 매핑하는 것, 모든 차수의 모든 다항식을 고려함  
`K(a, b) = exp(-y||a-b||**2) (y는 하이퍼 파라미터)`  

C가 클수록 하드마진, gamma가 클수록 훈련 데이터에 많이 의존(과적합에 주의)  

###### 결정 트리
데이터를 분류하거나 결괏값을 예측하는 분석 방법, 결과 모델이 트리구조  
사용 이유: 주어진 데이터에 대한 분류  
언제 사용할까?: 이상치가 많은 값으로 구성된 데이터셋을 다룰 때 사용하기, 결정 과정을 시각적으로 확인이 가능하여 머신 러닝이 어떤 방식으로 의사 결정을 하는지 알고 싶을 때 사용  

엔트로피: 확률 변수의 불확실성을 수치로 나타낸 것, 엔트로피가 높을수록 불확실성이 높다.  

[예시]  
엔트로피 = 0 = 불확실성 최소 = 순도 최대 
엔트로피 = 0.5 = 불확실성 최대 = 순도 최소  

동전을 두 번 던져 앞면이 나올 확률이 1/4이고, 뒷면이 나올 확률이 3/4일 때, 엔트로피는  
``` python
entropy(a) = -(1/4) * log(1/4) -(3/4) * log(3/4)  
= 2/4 + 3/4 * 0.42  
= 0.31  
```
지니계수: 불손도를 측정하는 지표로 데이터의 통계적 분산 정도를 정량화해서 표현한 값  
원소 n개 중에서 임의의 2개를 추출했을 때, 추출된 2개가 서로 다른 그룹에 속해 있을 확률을 의미합니다.  
`G(S) = 1 - 시그마 i부터 c까지 (pi)**2 (S: 이미 발생한 사건의 모음, c: 사건 개수)`  

로그를 사용하지 않아 계산이 빨라 결정 트리에서 많이 활용함

``` python
# 라이브러리 및 데이터 준비

import pandas as pd
df = pd.read_csv('./data/titanic/train.csv', index_col='PassengerId')
print(df.head())

# 데이터 전처리

# 승객의 생존 여부를 예측하기 위해 pclass, sex, age, sibsp, parch, fare을 사용한다.
df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]
# 성별을 나타내는 sex를 0 또는 1의 정수 값으로 반환
df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})
# 값이 없는 데이터 삭제
df = df.dropna()
X = df.drop('Survived', axis=1)
# Survived 값을 예측 레이블로 사용
y = df['Survived']

# 훈련과 테스트 데이터 셋으로 분리
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# 결정 트리 모델 생성
from sklearn import tree

model = tree.DecisionTreeClassifier()

# 모델 훈련

model.fit(X_train, y_train)

# 모델 예측
from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
# 테스트 데이터에 대한 예측 결과
accuracy_score(y_test, y_pred)

# 0.8268156424581006 = 약 83% -> 학습이 잘 되었다.

# 혼동 행렬을 위한 이용한 성능 측정
from sklearn.metrics import confusion_matrix

pd.DataFrame(
    confusion_matrix(y_test, y_pred),
    columns=['Predicted Not Survival', 'Predicted Survival'],
    index=['True Not Survival', 'True Survival']
)
```
혼동 행렬: 알고리즘 성능 평가에 사용한다.  

- True Positive: 모델의 예측과 실제가 True인 경우  
- True Negative: 모델의 예측은 True, 실제가 False인 경우  
- False Positive: 모델의 예측은 False, 실제가 True인 경우  
- False Negative: 모델의 예측과 실제가 False인 경우  

결정 트리를 좀 더 확대한 것(결정 트리를 여러 개 묶어놓은 것)을 랜덤 포레스트 기법이라고 한다.  

###### 로지스틱 회귀와 선형 회귀
회귀란 변수가 두 개 주어졌을 때, 한 변수에서 다른 변수를 예측하거나 두 변수의 관계를 규명하는 데 사용하는 방법  

- 독립 변수(예측 변수): 영향을 미칠 것으로 예상되는 변수  
- 종속 변수(기준 변수): 영향을 받을 것으로 예상되는 변수  

로지스틱 회귀: 분석하고자 하는 대상이 두 집단 혹은 그 이상의 집단으로 나누어질 경우, 개별 관측치가 어느 집단으로 분류할 수 있는지 분석하고, 이를 예측하는 모형을 개발하는 데 사용되는 통계 기법  

사용 이유: 주어진 데이터에 대한 분류
언제 사용할까?: 주어진 데이터에 대한 확신이 없거나(분류 결과에 대한 확신이 없을 때), 향후, 추가적으로 훈련 데이터셋을 수집하여 모델을 훈련할 수 있는 상황  

[분석 절차]  
1. 각 집단에 속하는 확률의 추정치를 예측합니다. 이때 추정치는 이진 분류의 경우 집단 1에 속하는 확률 P로 구합니다.
2. 분류 기준 값(cut-off)을 설정한 후 특정 범주로 분류합니다.  

``` python
# 라이브러리 호출 및 데이터 준비
%matplotlib inline
from sklearn.datasets import load_digits

# 숫자 데이터셋은 사이킷런에서 제공
digits = load_digits()

# digits 데이터셋의 형태(이미지가 1,797개 있으며  8x8 이미지의 64차원을 가짐)
print('Image Data Shape', digits.data.shape)

# 레이블(이미지의 숫자 정보) 이미지가 1,797개 있음
print('Label Data Shape', digits.target.shape)

# digits 데이터셋의 시각화

import numpy as np
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 4))
# 예시로 이미지 다섯 개만 확인
for index, (image, label) in enumerate(zip(digits.data[:5], digits.target[:5])):
    plt.subplot(1, 5, index+1)
    # cmap: 이미지 색상, reshpae: 이미지와 픽셀 크기, imshow: 이미지 그리기
    plt.imshow(np.reshape(image, (8, 8)), cmap=plt.cm.gray)
    plt.title(f'Training: {label}\n', fontsize=20)

# 훈련과 테스트 데이터셋 분리 및 로지스틱 회귀 모델 생성

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=0)
# 로지스틱 회귀 모델의 인스턴스 생성
logisticRegr = LogisticRegression()
# 모델 훈련
logisticRegr.fit(X_train, y_train)

# 일부 데이터를 활용한 모델 예측

# 새로운 이미지(테스트 데이터)에 대한 예측 결과를 넘파이 배열로 출력
logisticRegr.predict(X_test[0].reshape(1, -1))
# 이미지 10개에 대한 예측은 한 번에 배열로 출력
logisticRegr.predict(X_test[0:10])

# 전체 데이터를 사용한 모델 예측

# 전체 데이터셋에 대한 예측
predictions = logisticRegr.predict(X_test)
# 스코어 메서드를 사용한 성능 측정
score = logisticRegr.score(X_test, y_test)
print(score)

# 혼동 행렬 시각화
import numpy as np
import seaborn as sns
from sklearn import metrics

# 혼동 행렬
cm = metrics.confusion_matrix(y_test, predictions)
plt.figure(figsize=(9, 9))
# 히트맵으로 표현
sns.heatmap(cm, annot=True, fmt='.3f', linewidths=5, square=True, cmap='Blues_r')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
all_sample_title = f'Accracy Score {score:0f}'
plt.title(all_sample_title, size=15)
plt.show()
```

선형 회귀: 독립 변수 x를 사용하여 종속 변수 y의 움직임을 예측하고 설명하는 데 사용함  
독립 변수 x는 하나일 수도 있고(단순 선형 회귀), 여러 개일 수도 있음(다중 선형 회귀)  

[선형회귀와 로지스틱 회귀의 예시]
선형 회귀: 아이스크림이 한 시간에 몇 개가 팔리네~ 그러면 한 시간에 몇 개, 매출은 얼마나 되겠구나  
로지스틱 회귀: 이규석이가 아이스크림을 사는 확률을 계산하는 것  

선형 회귀는 직선을 출력, 로지스틱 회귀는 S-커브를 출력  

``` python
# 라이브러리 호출
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
# 주삐따 노트북에서 바로 matplotlib 이미지를 볼 수 있게 함
%matplotlib inline

# weather.csv 파일 불러오기

dataset = pd.read_csv('./data/weather.csv')

# 데이터 간 관계를 시각화

dataset.plot(x='MinTemp', y='MaxTemp', style='o')
plt.title('MinTemp vs MaxTemp')
plt.xlabel('MinTemp')
plt.ylabel('MaxTemp')
plt.show()

# 데이터를 독립 변수와 종속 변수로 분리하고 선형 회귀 모델 생성

X = dataset['MinTemp'].values.reshape(-1, 1)
y = dataset['MaxTemp'].values.reshape(-1, 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 선형 회귀 클래스
regressor = LinearRegression()
# fit() 메서드를 활용하여 모델 훈련
regressor.fit(X_train, y_train)

# 회귀 모델에 대한 예측
y_pred = regressor.predict(X_test)
df = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})
df

# 테스트 데이터셋을 사용한 회귀식 표현
plt.scatter(X_test, y_test, color='grey')
plt.plot(X_test, y_pred, color='red', linewidth=2)
plt.show()

# 선형 회귀 모델 평가
print(f'평균 제곱법: {metrics.mean_squared_error(y_test, y_pred)}')
print(f'루트 평균제곱법: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}')

# 루트 평균제곱법의 값 4.52는 모든 기운 백분율에 대한 평균값(22.41)과 비교하여 20% 정도라고 하면 모델 정확도는 높지 않지만, 합리적으로 좋은 예측을 하고 있다.
```
##### 비지도 학습
지도 학습처럼 레이블이 필요하지 않으며 정답이 없는 상태에서 훈련시키는 방식  

- 군집: 각 데이터의 유사성(거리)를 측정한 후, 유사성이 높은 데이터끼리 집단으로 분류하는 것  
- 차원 축소: 차원을 나타내는 특성을 줄여서 데이터를 줄이는 방식  

###### K-평균 군집화
데이터를 입력받아 소수의 그룹으로 묶는 알고리즘  

사용 이유: 주어진 데이터 군집화  
언제 사용할까?: 데이터셋을 이용하여 몇 개의 클러스터를 구성할 지 사전에 알 수 있을 때  

[학습 과정]  
1. 중심점 선택
2. 클러스터 할당
3. 새로운 중심점 선택
4. 범위 확인  

[K-군집화 알고리즘을 사용하기 적합하지 않은 경우]  
- 데이터가 비선형일 때  
- 군집의 크기가 다를 때 
- 군집마다 밀집도와 거리가 다를 때  

K-군집화 알고리즘의 핵심은 적절한 K 값을 찾는 것  

``` python
# 라이브러리 호출
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 상품에 대한 연 지출 데이터 sales data.csv 호출

data = pd.read_csv('./data/sales data.csv')
data.head()

# 연속형 데이터와 명목형 데이터로 분류

# 명목형 데이터
categorical_features = ['Channel', 'Region']
# 연속형 데이터
continuous_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']

for col in categorical_features:
    # 명목형 데이터는 판다스의 get_dummies() 메서드를 사용하여 0과 1로 변환
    # prefix: 새로운 행에 접두사 붙여버리면서
    dummies = pd.get_dummies(data[col], prefix=col)
    # 가로로 붙여라
    data = pd.concat([data, dummies], axis=1)
    # 원래 channel, region 떨구기
    data.drop(col, axis=1, inplace=True)
data.head()

# 데이터 전처리(스케일링 적용)

# 0과 1 사이로 스케일링
mms = MinMaxScaler()
mms.fit(data)
data_transformed = mms.transform(data)

# 적당한 K 값 추출

Sum_of_squared_distances = []
K = range(1, 15)
for k in K:
    # 1~14의 K 값 적용
    km = KMeans(n_clusters=k)
    # KMeans 모델 훈련
    km = km.fit(data_transformed)
    # km.inertia_: 모델의 관성 값, 작을수록 클러스터간 분리가 잘 되어 있다.
    Sum_of_squared_distances.append(km.inertia_)

# blue, marker, 선을 그려라
plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Optimal k')
plt.show()
```
거리 제곱의 합은 x, y 두 데이터의 차를 구해서 제곱한 값을 모두 더한 후, 유사성을 측정하는 데 사용   

K가 증가하면 거리 제곱의 합은 0이 되는 경향이 있다.  
K가 요소의 수가 같아지면 요소의 수가 하나의 클러스터를 각각 형성하기 때문에  

그러면 우리는 이제 K가 급격하게 0에 가까워지는 지점이 이상적인 K 값이라고 할 수 있다.  
위에 그래프에서는 K=7일 0과 가까워지므로 k=5가 적절하다고 할 수 있다.  

###### 밀도 기반 군집 분석
일정 밀도 이상을 가진 데이터를 기준으로 군집을 형성하는 방법  

사용 이유: 주어진 데이터에 대한 군집화  
언제 사용할까?:  K-평균 군집화와 다르게 사전에 클러스터의 숫자를 알지 못할 때 사용하면 유용함, 또한 주어진 데이터에 이상치가 많이 포함되었을 때 사용하면 좋다.  

[군집 방법 절차]  
1. 엡실론 내 점 개수 확인 및 중심점 결정  
2. 군집 확장(새로운 군집을 확장하고 합친다.)  
3. 1,2 단계 반복  
4. 노이즈 정의  

###### 주성분 분석(PCA)
고차원 데이터를 저차원으로 축소하여 데이터가 가진 대표 특성을 추출하여 성능과 작업에서 이점을 보는 알고리즘  
비선형 데이터를 처리할 때는 일부로 차원을 늘리기도 한다.  
왜 why? 분석하기 어려우니까  

사용 이유: 주어진 데이터의 간소화  
언제 사용할까?: 현재 데이터의 특성이 많은 경우 데이터를 하나의 플롯(plot)에 시각화하여 살펴보기  

[차원 축소 방법]  
1. 데이터의 분포 특성을 잘 설명하는 백터를 두 개 선택  
2. 벡터 2개를 위한 적절한 가중치를 찾을 때까지 학습을 진행  

PCA는 데이터 하나하나의 성분을 분석하는 것이 아니라 여러 데이터가 모여 하나의 분포를 만들 때, 분포의 주성분을 분석하는 것이다.  

``` python
# 라이브러리 호출

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 밀도 기반 군집 분석 
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import normalize
# 데이터 차원 축소
from sklearn.decomposition import PCA

# 데이터 불러오기

X = pd.read_csv('./data/credit card.csv')
print(X)
# cust_id 열을 삭제
X = X.drop('CUST_ID', axis=1)
# 바로 위의 결측값과 동일한 것으로 채워라
X.ffill(inplace=True)
print(X.head())

# 데이터 전처리 및 데이터를 2차원으로 차원 축소
scaler = StandardScaler()
# 평균이 0, 표준 편차가 1이 되도록 데이터 크기를 조정
X_scaled = scaler.fit_transform(X)

# 데이터가 가우스 분포를 따르도록 정규화
X_normalized = normalize(X_scaled)
# 넘파이 배열을 데이터프레임으로 변환
X_normalized = pd.DataFrame(X_normalized)

# 2차원으로 차원 축소 선언
pca = PCA(n_components=2)
# 차원 축소 적용
X_principal = pca.fit_transform(X_normalized)
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']
print(X_principal.head())

# DBSCAN 모델 생성 및 결과의 시각화

# 모델 생성 및 훈련
# eps: 최대 탐색 거리, min_sample: 클러스터로 구성하기 위한 최소 샘플 개수
db_default = DBSCAN(eps=0.0375, min_samples=3).fit(X_principal)
# 각 데이터 포인트에 할당된 모든 클러스트 레이블의 넘파이 배열을 label에 저장
labels = db_default.labels_

# 출력 그래프의 색상을 위한 레이블 생성
colours = {}
colours[0] = 'y'
colours[1] = 'g'
colours[2] = 'b'
colours[-1] = 'k'

# 각 데이터 포인트에 대한 색상 벡터 생성
cvec = [colours[label] for label in labels]

# 플롯의 범례 구성
r = plt.scatter(X_principal['P1'], X_principal['P2'], color='y')
g = plt.scatter(X_principal['P1'], X_principal['P2'], color='g')
b = plt.scatter(X_principal['P1'], X_principal['P2'], color='b')
k = plt.scatter(X_principal['P1'], X_principal['P2'], color='k')

plt.figure(figsize=(9, 9))
# 정의된 색상 벡터에 따라 X축에 P1, Y축에 P2 플로팅
plt.scatter(X_principal['P1'], X_principal['P2'], c=cvec)

# 범례 구축
plt.legend((r, g, b, k), ('Label 0', 'Label 1', 'Label 2', 'Label -1'))
plt.show()

# 모델 튜닝

db = DBSCAN(eps=0.0375, min_samples=50).fit(X_principal)
labels1 = db.labels_

colours1 = {}
colours1[0] = 'r'
colours1[1] = 'g'
colours1[2] = 'b'
colours1[3] = 'c'
colours1[4] = 'y'
colours1[5] = 'm'
colours1[-1] = 'k'

cvec = [colours1[label] for label in labels1]
colors1 = ['r', 'g', 'b', 'c', 'y', 'm', 'k']

r = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[0])
g = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[1])
b = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[2])
c = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[3])
y = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[4])
m = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[5])
k = plt.scatter(X_principal['P1'], X_principal['P2'], marker='o', color=colors1[6])

plt.figure(figsize=(9, 9))
plt.scatter(X_principal['P1'], X_principal['P2'], c=cvec)
plt.legend((r, g, b, c, y, m, k),
('Label 0', 'Label 1', 'Label 2', 'Label 3', 'Label 4', 'Label 5', 'Label -1'),
# 점 하나만 표시
scatterpoints=1,
loc='upper left',
# 3열로 표시
ncol=3,
fontsize=8)

plt.show()

# min_samples를 50에서 100으로 변경

db = DBSCAN(eps=0.0375, min_samples=100).fit(X_principal)

# 이렇게 모델의 하이퍼파라미터를 변경하면 클러스터 결과가 달라지므로 최적의 성능을 내기 위해서는 모델 튜닝이 중요하다. 
```