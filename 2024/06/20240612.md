# 20240612
## AI 드론봇 - 10

#### 자연어 처리를 위한 임베딩

##### 임베딩
사람이 사용하는 언어(자연어)를 컴퓨터가 이해할 수 있는 언어(숫자) 형태인 벡테로 변환한 결과 혹은 일련의 과정을 의미합니다.

[역할]  
- 단어 및 문장 간 관련성 계산
- 의미적 혹은 문법적 정보의 압축(ex. 왕-여왕, 교사-학생)  

###### 희소 표현 기반 임베딩
희소 표현(sqarse representation): 대부분의 값이 0으로 채워져 있는 경우

원-핫 인코딩: 주어진 텍스트를 숫자(벡터)로 변환해주는 것  
단어 N개를 N차원의 벡터로 표현하는 방식, 단어 포함되어 있는 위치에 1을 넣고, 나머지에는 0의 값을 채우는 것  

[원-핫 인코딩의 치명적인 단점]  
1. 희소 벡터의 내적은 0의 값을 가지므로 직교를 이룬다.  
> 단어끼리의 관계성(유의어, 반의어)가 없이 서로 독립적인 관계가 된다.  

2. 차원의 저주: 하나의 단어를 표현하는 데 말뭉치에 있는 수만큼 차원이 생성된다.  

> 이 대안으로 신경망에 기반하여 단어를 벡터로 바꾸는 방법론이 주목을 받고 있음  
ex. 워드투벡터, 글로브, 패스트텍스트 등  

###### 횟수 기반 임베딩
단어가 출현한 빈도를 고려하여 임베딩하는 방법  

카운터 벡터: 문서 집합에서 단어를 토큰으로 생성하고 각 단어의 출현 빈도수를 이용하여 인코딩해서 벡터를 만드는 방법  
> 토크나이징과 벡터화가 동시에 가능함  

`CountVectorizer()`: 문서를 토큰 리스트로 변환, 각 문서에서 토큰의 출현 빈도 세기, 각 문서를 인코딩하고 벡터로 변환  

TF-IDF(Term Frequency-Inverse Document Frequency): 정보 검색론에서 가중치를 구할 때 사용하는 알고리즘  
  - TF(단어 빈도): 문서 내에서 특정 단어가 출현한 빈도를 의미  
  - DF(문서 빈도): 한 단어에 전체 문서가 얼마나 공통적으로 많이 등장하는지 나타내는 값, 특정 단어가 나타난 문서의 개수  
  - IDF(역문서 빈도): 문서 빈도의 역수, 전체 문서의 개수가 많아지면 IDF 값이 커지므로 log를 취해줌, 분모가 0이 되는 상황을 방지하고자 분모에 1을 더해주는 스무딩을 적용함

[TF-IDF의 활용]  
- 키워드 검색을 기반으로 하는 검색 엔진
- 중요 키워드 분석
- 검색 엔진에서 검색 결과의 순위를 지정

TF-IDF 값은 특성 문서 내의 단어의 출현 빈도가 높거나 전체 문서에서 특정 단어가 포함된 문서가 적을수록 TF-IDF 값이 높다.  
> a, the, is 같이 흔한 단어는 걸래낼 수 있고, 특정 단어에 대한 중요도를 찾을 수 있다.  

###### 예측 기반 임베딩
신경망 구조 혹은 모델을 이용하여 특정 문맥에서 어떤 단어가 나올지 예측하면서 단어를 벡터로 만드는 방식  

워드투벡터: 신경망 알고리즘, 주어진 텍스트에서 텍스트의 각 단어마다 하나씩 일련의 벡터를 출력  
위드투벡터의 출력 벡터가 2차원 그래프에 표시될 때, 의미론적으로 유사한 단어의 벡터는 서로 가깝게 표현  
코사인 유사도를 통해 단어 간의 거리를 측정하여 관계성을 나타낸다.  
> 워드투벡터를 통해 특정 단어의 동의어를 찾을 수 있다.  

[수행 과정]  
1. 일정한 크기의 윈도우로 분할된 텍스트를 신경망 입력으로 사용
윈도우는 하나의 어절로 구성  
2. 분할된 텍스트는 한 쌍의 대상 단어와 컨텍스트로 네트워크에 공급
3. 대상 단어를 입력하면 출력으로 다른 단어와의 관계가 출력됨

CBOW(Continuous Bag of Words): 단어를 여러 개 나열한 후 이와 관련된 단어를 추정하는 방식  
> 문장에서 등장하는 n개의 단어 열에서 다음에 등장할 단어를 예측  
ex. calm cat slept on the sofa 라는 문장이 있을 때, 'calm cat on the sofa'라는 문맥이 주어지면 slept를 예측하는 것  

[`Word2Vec`의 파라미터]  
- 첫 번째 파라미터: CBOW를 적용할 데이터셋
- min_count: 단어에 대한 최소 빈도수 제한(빈도가 적은 단어는 학습하지 않음)
- vector_size: 워드 벡터의 특징 값, 즉, 임베딩된 벡터의 차원
- window: 컨텍스트 원도우 크기
- sg: sg가 0일 때는 CBOW를 의미하며, sg가 1일 때는 skip-gram을 나타냄, 값을 지정하지 않으면 기본 값은 CBOW를 의미

워드투벡터는 무작위로 초기화되고, 훈련 과정에서도 무작위 처리되기에 결과가 항상 같지 않을 수도 있다.  
[참고 Q11을 참고](https://github.com/piskvorky/gensim/wiki/Recipes-&-FAQ)  

skip-gram: CBOW 방식과 반대로 특정한 단어에서 문맥이 될 수 있는 단어를 예측하는 것  
ex. 중심 단어 'slept'를 이용하여 그 앞과 뒤의 단어를 유추하는 것  

CBOW와 skip-gram 중 어떤 알고리즘이 더 좋다고 결론을 내리기보다 분석하고자 하는 데이터 성격, 분석에 대한 접근 방법 및 도출하고자 하는 결론을 종합적으로 고려하여야 함  

패스트텍스트: 워드투벡터의 단점을 보완하고자 페이스북에서 개발한 임베딩 알고리즘  
기존 워드투벡터의 워드 임베딩 방식은 분산 표현을 이용하여 단어의 분산 분포가 유사한 단어에 비슷한 벡터 값을 할당하여 표현  
패스트텍스트는 단어 표현 방법을 사용함, 노이즈에 강하며 새로운 단어에 대해서는 형태적 유사성을 고려한 벡터 값  

[패스트텍스트가 워드투벡터 단점을 극복하는 방법]  
- 사전에 없는 단어에 벡터 값을 부여하는 방법  
  1. 패스트텍스트는 주어진 문서의 각 단어를 n-그램으로 표현, n의 값에 따라서 문장을 n어절로 나눔  
  n이 1이면, unigram, 2이면, bigram, 3이면, trigram
  2. 인공 신경망을 이용하여 학습이 완료된 후 데이터셋의 모든 단어를 각 n-그램에 대해 임베딩함
    > 사전에 없는 단어가 등장하면 n-그램으로 분리된 부분 단어와 유사도를 계산하여 의미를 유추할 수 있음

- 자주 사용되지 않는 단어에 학습 안정성을 확보하는 방법
  + 워드투벡터는 단어의 출현 빈도가 적으면 임베딩의 정확도가 낮다.  
  왜 why? 참고할 수 있는 경우의 수가 적기 때문에 상대적으로 정확도가 낮기 때문에  
  하지만, 패스트텍스트는 등장 빈도수가 적더라도 n-그램으로 임베딩하기 때문에 참고할 수 있는 경우의 수가 많다.  
  > 워드투벡터에 비해 상대적으로 자주 사용되지 않는 단어에서도 정확도가 높다.  

FastText에서 사용하는 파라미터는 Word2Vec과 같음  
  - 첫 번째 파라미터: 패스트 텍스트에 적용할 데이터셋
  - vector_size: 학습할 임베딩의 크기, 임베딩된 벡터의 차원
  - window: 고려할 앞뒤 폭(앞 뒤 세 단어)
  - min_count: 단어에 대한 최소 빈도수 제한(1회 이하 단어 무시)
  - epochs: 반복 횟수(이전에는 iter를 사용했지만, 현재는 epochs로 변경)

###### 횟수/예측 기반 임베딩

횟수 기반과 예측 기반의 단점을 보완하기 위한 임베딩 기법은 대표적으로 글로브가 있음  

글로브(Global Vectors for Word Representtation)는 횟수 기반의 LSA(Latent Semantic Analysis, 잠재 의미 분석)과 예측 기반의 워드투벡터 단점을 보완하기 위한 모델, 단어에 대한 글로벌 동시 발생 확률 정보를 포함하는 단어 임베딩 방법  

> 단어에 대한 통계 정보와 skip-gram 방법을 사용하고, 통계적 기법이 추가  

`datapath('경로')`: 파일을 메모리로 로딩, `glove.6B.100d.txt`는 수많은 단어에 대해 차원 100개를 가지는 임베딩 벡터를 제공  
`glove2word2vec()`: glove를 워드투벡터 형태로 변경 가능, 첫 번째 인자: 글로브 입력 파일, 두 번째 인자: 워드투벡터 출력 파일  
  > 글로브 데이터를 워드투벡터로 변환하겠다는 것  


##### 트랜스포머 어텐션
어텐션(attention)은 주로 영어 번역에서 사용되기 때문에 인코더와 디코더 네트워크를 사용합니다.  
> 입력에 대한 벡터 변환을 인코더에서 처리하고 모든 벡터를 디코더로 보냄  
왜 why?: 시간이 흐를수록 초기 정보를 잃어버리는 기울기 소멸 문제를 해결하기 위함  
그러나 모든 벡터가 전달되므로 행렬 크기가 굉장히 커진다.  
소프트맥스 함수로 가중합을 구하고, 그 값을 디코더에 전달함  

디코더는 은닉 상태에 대해 중점적으로 집중해서 보아야 할 벡터를 소프트맥스 함수로 점수를 매긴 후, 각각의 은닉 상태의 벡터와 곱함  
> 어텐션은 모든 벡터 중에서 꼭 살펴보아야 할 벡터에 집중하겠다는 의미  

트랜스포머는 2017년 6월에 논문을 통해 발표된 것으로 어텐션을 극대화한 것  
어텐션에서는 네트워크가 하나씩이지만, 트랜스포머는 인코더와 디코더를 여러 개 중첩시킨 구조  
각각의 인코더와 디코더를 블록이라고 함  

모든 블록의 구조는 동일하다.  
하나의 인코더는 셀프 어텐션과 전방향 신경망으로 구성  
인코더는 단어를 벡터로 임베딩 -> 셀프 어텐션으로 그리고 전방향 신경망으로 전달  
셀프 어텐션은 문장에서 각 단어끼리 얼마나 관계가 있는지 계산해서 반영함  
> 셀프 어텐션을 문장 안에서 단어 간 관계를 파악할 수 있음  

디코더는 층을 3개 가지고 있는데 인코더에서 넘어온 벡터가 처음으로 만나는 것이 셀프 어텐션 층 -> 그곳을 지나면 인코더-디코더 어텐션 층이 있다. 인코더-디코더 어텐션 층은 인코더가 처리한 정보를 받아 어텐션 메커니즘을 수행하고, 마지막으로 전방향 신경망으로 데이터가 전달  

어텐션 스코어: 현재 디코더의 시점 i에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 값이 디코더의 현 시점의 은닉 상태 값이 디코더의 현 시점의 은닉 상태와 얼마나 관련이 있는지 판단하는 값  

어텐션 스코어가 계산되었으면 소프트맥스 함수를 적용하여 확률로 변환 -> 특성 시점에 대한 가중치(시간의 가중치)  

컨텍스트 벡터: 시간의 가중치와 은닉 상태의 가중합을 계산한 하나의 벡터

디코더의 은닉 상태: 컨텍스트 벡터와 디코더 이전 시점의 은닉 상태와 출력이 필요  
어텐션이 적용된 인코더-디코더 수식은 컨텍스트 벡터가 계속 변함  
반면, 어텐션이 적용되지 않은 인코더-디코더 수식은 컨텍스트 벡터가 고정

[정리]  
목적: They are cats라는 입력 시퀀스에 대해 현 시점의 디코더 은닉 상태와 출력을 계산하기  
1. 디코더 은닉 상태와 모든 은닉 상태에 대한 어텐션 스코어를 계산
2. 어텐션 스코어를 구했으면 소프트맥스 함수를 적용하여 시간의 가중치를 구함
3. 시간의 가중치와 인코더의 은닉 상태 값을 이용하여 가중합을 계산하여 컨텍스트 벡터를 구함
4. 앞에서 구한 컨텍스트 벡터와 디코더 이전 시점의 은닉 상태와 출력을 이용하여 최종적으로 다음 디코더의 은닉 상태를 출력

###### seq2seq
`seq2seq(sequence to sequence)`는 입력 시퀀스에 대한 출력 시퀀스를 만들기 위한 모델, 품사 판별과 같은 시퀀스 레이블링과는 차이가 존재  
입력이 있으면 출력이 되는 형태, 입력과 출력에 대한 문자열이 같음  

`seq2seq`는 품사 판별보다는 번역에 초점을 맞춘 모델  
번역: 입력 시퀀스의 x과 의미가 동일한 출력 시퀀스를 만드는 것, x, y간의 관계는 중요하지 않음  
각 시퀀스의 길이도 서로 다를 수 있다.  

`__future__`: 구 버전에서 상위 버전의 기능을 이용해야 할 때 사용  
`re`: 정규 표현식을 이용하고자 할 때 사용  
정규표현식: 특정한 규칙을 갖는 문자열의 집합을 표현하기 위한 형식, 복잡한 문자열의 검색과 치환을 위해 주로 사용  

``` python
import re
com = re.compile('[cats]')
com.findall('I love cats')

"""
['c', 'a', 't', 's']
"""
```

`pd.read_csv()`의 파라미터  
- delimiter: CSV 파일의 데이터가 어떤 형태(\t, +, ' ')로 나누어져 있는지 확인, 가령, 콤마를 기준으로 하면 "Sure, I'm" 이라는 문장은 Sure과 I'm으로 나뉘니 여기서는 탭을 기준으로 하여 적절하게 분할하였다.  

파이토치에서 seq2seq 모델을 사용하기 위해서는 인코더와 디코더에 대한 정의 필요  
인코더와 디코더를 사용하면 문장의 번역뿐만 아니라 다음 입력 예측이 가능함, 각 문장의 끝에는 끝을 알리는 토큰이 할당됨  


문장: 나는 고양이를 좋아한다.  
[인코더]  
임베딩 계층: 입력에 대한 임베딩 결과가 저장되어 있는 딕셔너리를 조회하는 테이블  
GRU 계층: 연속하여 들어오는 입력을 계산, 이전 계층의 은닉 상태를 계산한 후, 망각 게이트와 업데이트 게이트를 갱신  

[디코더]  
디코더: Null I love cats  
임베딩: 출력을 위해 딕셔너리를 조회할 테이블 생성  
GRU: 다음 단어를 예측하기 위한 확률 계산  
소프트맥스: I love cats ., 계산된 확률 값 중 최적의 값(최종 출력 단어)를 선택하기 위해 소프트맥스 활성화 함수를 사용  

소프트맥스: 일정한 시퀀스의 숫자를 0과 1사이의 양의 수로 변환하여 클래스의 확률을 구할 때 사용  
로그 소프트맥스: 소프트맥스와 로그 함수의 결합, 소프트맥스 활성화 함수에서 발생할 수 있는 기울기 소멸 문제를 방지하기 위해 만들어진 활성화 함수  

티처포스: 번역(예측)하려는 목표 단어(ground truth)를 디코더의 다음 입력으로 넣는 기법  
쉽게 말해 정답을 그냥 바로 다음 입력 값에 넣는 방법이다.  

티처포스 사용의 장점: 학습 초기 안정적인 훈련, 기울기를 계산할 때 빠른 수렴  
티처포스 사용의 단점: 네트워크가 불안정해질 수 있음  

NLLLoss: 크로스엔트로피 손실 함수(CrossEntropyLoss)와 마찬가지로 분류 문제에 사용  

크로스엔트로피 손실 함수 = 소프트맥스 함수 + NLLLoss  
> NLLLoss에서 소프트맥스 함수를 사용하려면 명시해줘야 함  

###### 버트(BERT)

##### 한국어 임베딩
